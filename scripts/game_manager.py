#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¸æˆç®¡ç†å™¨ - ç»Ÿä¸€çš„æ¸¸æˆæ•°æ®ç®¡ç†å·¥å…·
åŒ…å«ï¼šçˆ¬è™«ã€æ¸…ç†ã€å»é‡ã€ä¿®å¤ã€ä¸‹è½½ç­‰æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½

ğŸ†• æ–°åŠŸèƒ½ï¼šæ™ºèƒ½é€‰æ‹©å™¨æ£€æµ‹
- ç°åœ¨å¯ä»¥è‡ªåŠ¨æ£€æµ‹æ¸¸æˆç½‘ç«™çš„CSSé€‰æ‹©å™¨
- æ·»åŠ æ–°å¹³å°åªéœ€è¦æä¾›åŸºæœ¬URLä¿¡æ¯
- æµ‹è¯•å·¥å…·ï¼špython test_selector_detection.py <ç½‘ç«™URL>

ğŸ”§ é…ç½®æ–¹æ³•ï¼š
1. ç›´æ¥ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰ï¼š
   ç¼–è¾‘ scripts/config.py ä¸­çš„ Config ç±»ï¼Œå¦‚ USE_PROXY = True

2. ç¯å¢ƒå˜é‡é…ç½®ï¼š
   export USE_PROXY=true PROXY_HOST=127.0.0.1 PROXY_PORT=7890

3. å‘½ä»¤è¡Œå‚æ•°ï¼š
   python game_manager.py --use-proxy --strict-whitelist

ğŸ“‹ æŸ¥çœ‹å½“å‰é…ç½®ï¼špython show_config.py

ğŸ“ é‡è¦é…ç½®é¡¹ï¼š
- USE_PROXY: æ˜¯å¦å¯ç”¨ä»£ç†
- STRICT_WHITELIST: æ˜¯å¦å¯ç”¨ä¸¥æ ¼ç™½åå•æ¨¡å¼
- SERPAPI_KEY/GOOGLE_API_KEY: APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰

è¯¦ç»†é…ç½®è¯´æ˜è¯·æŸ¥çœ‹ WHITELIST_MODES.md
"""

import requests
from bs4 import BeautifulSoup
import json
import random
import time
import os
import zipfile
import shutil
import logging
import re
from urllib.parse import urljoin, urlparse
from datetime import datetime
from typing import List, Dict, Optional, Any
import argparse
from http.client import RemoteDisconnected
from requests.exceptions import RequestException, ConnectionError, Timeout
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type

# å°è¯•å¯¼å…¥SerpAPI
try:
    import serpapi
    SERPAPI_AVAILABLE = True
except ImportError:
    SERPAPI_AVAILABLE = False
    print("âš ï¸ SerpAPIåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€APIè°ƒç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('game_manager.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é¡¹ç›®é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GAMES_DATA_FILE = os.path.join(PROJECT_ROOT, 'src', 'data', 'games.ts')
LOCAL_GAMES_DIR = os.path.join(PROJECT_ROOT, 'public', 'games')
THUMBNAILS_DIR = os.path.join(PROJECT_ROOT, 'public', 'games', 'thumbnails')



# å¯¼å…¥å…±äº«é…ç½®
from config import Config, USE_PROXY, PROXY_HOST, PROXY_PORT, STRICT_WHITELIST, SERPAPI_KEY, GOOGLE_API_KEY, GOOGLE_CX

# å…¨å±€ä»£ç†è®¾ç½®ï¼ˆç±»ä¼¼Javaçš„-Dhttps.proxyHostï¼‰
def setup_global_proxy():
    """è®¾ç½®å…¨å±€ä»£ç†ï¼Œç±»ä¼¼Javaçš„JVMå‚æ•°"""
    if USE_PROXY:
        proxy_url = f"http://{PROXY_HOST}:{PROXY_PORT}"
        
        # æ–¹æ³•1ï¼šç¯å¢ƒå˜é‡ï¼ˆå¯¹æ‰€æœ‰HTTPåº“ç”Ÿæ•ˆï¼‰
        os.environ['HTTP_PROXY'] = proxy_url
        os.environ['HTTPS_PROXY'] = proxy_url
        os.environ['http_proxy'] = proxy_url
        os.environ['https_proxy'] = proxy_url
        
        # æ–¹æ³•2ï¼šä¸ºrequestsåº“è®¾ç½®å…¨å±€é»˜è®¤ä»£ç†
        import requests
        requests.adapters.DEFAULT_RETRIES = 3
        
        # æµ‹è¯•ä»£ç†æ˜¯å¦å¯ç”¨
        try:
            test_response = requests.get('https://httpbin.org/ip', 
                                       proxies={'http': proxy_url, 'https': proxy_url}, 
                                       timeout=5)
            if test_response.status_code == 200:
                logger.info(f"âœ… ä»£ç†æµ‹è¯•æˆåŠŸ: {PROXY_HOST}:{PROXY_PORT}")
                return True
            else:
                logger.warning(f"âš ï¸ ä»£ç†æµ‹è¯•å¤±è´¥: HTTP {test_response.status_code}")
                return False
        except Exception as e:
            logger.warning(f"âš ï¸ ä»£ç†ä¸å¯ç”¨: {e}")
            return False
    return False

# åˆå§‹åŒ–å…¨å±€ä»£ç†
PROXY_AVAILABLE = setup_global_proxy() if USE_PROXY else False

# æ¨¡æ‹Ÿæµè§ˆå™¨å¤´ï¼ˆè½®æ¢ä½¿ç”¨ï¼‰
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_random_headers():
    """è·å–éšæœºçš„è¯·æ±‚å¤´"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    }

# é«˜è´¨é‡HTML5æ¸¸æˆå¹³å°
# ğŸ“ ç°åœ¨æ”¯æŒæ™ºèƒ½é€‰æ‹©å™¨æ£€æµ‹ï¼å¯ä»¥ä¸å¡«å†™game_selectorå’Œtitle_selectorï¼Œè„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹
PREMIUM_GAME_SITES = [
    {
        'name': 'itch.io HTML5',
        'base_url': 'https://itch.io',
        'search_url': 'https://itch.io/games/html5',
        # å¯é€‰ï¼šæ‰‹åŠ¨æŒ‡å®šé€‰æ‹©å™¨ï¼ˆå¦‚æœè‡ªåŠ¨æ£€æµ‹ä¸å‡†ç¡®ï¼‰
        'game_selector': '.game_cell',
        'title_selector': '.title',
        'priority': 1
    },
    {
        'name': 'GameJolt',
        'base_url': 'https://gamejolt.com',
        'search_url': 'https://gamejolt.com/games',
        # è®©è„šæœ¬è‡ªåŠ¨æ£€æµ‹é€‰æ‹©å™¨
        'priority': 2
    },
    # ğŸ“ æ·»åŠ æ–°å¹³å°ç°åœ¨æ›´ç®€å•äº†ï¼åªéœ€è¦æä¾›åŸºæœ¬ä¿¡æ¯ï¼š
    {
        'name': 'å¹³å°åç§°',
        'base_url': 'https://www.crazygames.com',
        'search_url': 'https://www.crazygames.com',
        'priority': 3
    }
    # è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹æ¸¸æˆåˆ—è¡¨çš„CSSé€‰æ‹©å™¨ï¼
    
    # ç¤ºä¾‹ï¼šCrazyGamesï¼ˆè‡ªåŠ¨æ£€æµ‹é€‰æ‹©å™¨ï¼‰
    # {
    #     'name': 'CrazyGames',
    #     'base_url': 'https://www.crazygames.com',
    #     'search_url': 'https://www.crazygames.com/c/html5',
    #     'priority': 3
    # },
    
    # ç¤ºä¾‹ï¼šKongregateï¼ˆè‡ªåŠ¨æ£€æµ‹é€‰æ‹©å™¨ï¼‰
    # {
    #     'name': 'Kongregate',
    #     'base_url': 'https://www.kongregate.com',
    #     'search_url': 'https://www.kongregate.com/games',
    #     'priority': 4
    # }
]

# å¯åµŒå…¥åŸŸåç™½åå•ï¼ˆçœŸæ­£çš„æ¸¸æˆæ‰˜ç®¡åŸŸåï¼‰
EMBEDDABLE_DOMAINS = [
    # itch.io æ¸¸æˆæ‰˜ç®¡åŸŸå
    'html-classic.itch.zone',
    'v6p9d9t4.ssl.hwcdn.net',
    'kdata.itch.zone',
    'assets.itch.zone',
    
    # Newgrounds æ¸¸æˆåŸŸå
    'uploads.ungrounded.net',
    'www.newgrounds.com/portal/view',
    'newgrounds.com/portal',
    
    # GameJolt æ¸¸æˆåŸŸå
    'gamejolt.net',
    'cdn.gamejolt.net',
    
    # å…¶ä»–çŸ¥åæ¸¸æˆå¹³å°
    'crazygames.com/embed',
    'embed.crazygames.com',
    'poki.com/embed',
    'embed.poki.com',
    'kongregate.com/games',
    'armor.ag/onstage',
    'www.kongregate.com/games',
    
    # HTML5æ¸¸æˆCDN
    'html5.gamedistribution.com',
    'game-cdn.poki.com',
    'assets.crazygames.com',
    
    # ğŸ“ åœ¨è¿™é‡Œæ·»åŠ æ–°å‘ç°çš„æ¸¸æˆæ‰˜ç®¡åŸŸå
    # ä¾‹å¦‚ï¼š
    'crazygames.com/new',
    # 'games.miniclip.com',
    # 'embed.armorgames.com'
]

# APIæœç´¢æŸ¥è¯¢è¯ï¼ˆé’ˆå¯¹åœ¨çº¿å¯ç©æ¸¸æˆä¼˜åŒ–ï¼‰
GAME_SEARCH_QUERIES = [
    # é’ˆå¯¹ç‰¹å®šå¹³å°çš„iframeæ¸¸æˆ
    'site:itch.io "play in browser" iframe -forum -discussion -devlog',
    'site:gamejolt.com "play now" HTML5 -community -forum',
    'site:newgrounds.com "play online" -forum -review',
    'site:crazygames.com "play online" HTML5 -blog',
    'site:poki.com games "play online" -blog -news',
    
    # é€šç”¨åœ¨çº¿æ¸¸æˆæœç´¢ï¼ˆæ’é™¤éæ¸¸æˆå†…å®¹ï¼‰
    '"play now" "in browser" HTML5 -download -forum -wiki -review -blog',
    '"browser game" iframe embed "play online" -tutorial -guide',
    'HTML5 games "no download" embed -forum -discussion -blog',
    '"web game" iframe "play instantly" -review -news -forum',
    '"online game" HTML5 canvas "play free" -download -app store'
]

# ä»£ç†æ± ï¼ˆå¯é€‰ï¼‰
PROXY_LIST = []

# ç§»é™¤å¤æ‚çš„ProxyManagerç±»ï¼Œä½¿ç”¨å…¨å±€ä»£ç†è®¾ç½®

class GameManager:
    """ç»Ÿä¸€çš„æ¸¸æˆç®¡ç†å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(LOCAL_GAMES_DIR, exist_ok=True)
        os.makedirs(THUMBNAILS_DIR, exist_ok=True)
        
        # æ£€æŸ¥APIé…ç½®
        self.has_serpapi = bool(SERPAPI_KEY)
        self.has_google_api = bool(GOOGLE_API_KEY and GOOGLE_CX)
        
        if self.has_serpapi:
            logger.info("âœ… SerpAPI å·²é…ç½®")
        if self.has_google_api:
            logger.info("âœ… Google Custom Search API å·²é…ç½®")
        if not (self.has_serpapi or self.has_google_api):
            logger.info("âš ï¸ æœªé…ç½®APIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸºç¡€çˆ¬è™«åŠŸèƒ½")
        
        # æ˜¾ç¤ºä»£ç†çŠ¶æ€
        if USE_PROXY:
            if PROXY_AVAILABLE:
                logger.info(f"âœ… å…¨å±€ä»£ç†å·²å¯ç”¨: {PROXY_HOST}:{PROXY_PORT}")
                logger.info("  - ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œæ‰€æœ‰HTTPè¯·æ±‚å°†èµ°ä»£ç†")
            else:
                logger.info("âš ï¸ ä»£ç†é…ç½®å¤±è´¥ï¼Œå°†ç›´æ¥è¿æ¥")
        else:
            logger.info("â„¹ï¸ ä»£ç†æ¨¡å¼æœªå¯ç”¨ï¼Œç›´æ¥è¿æ¥ç½‘ç»œ")
        
        # æ˜¾ç¤ºåçˆ¬è™«ç­–ç•¥çŠ¶æ€
        logger.info("ğŸ›¡ï¸ åçˆ¬è™«ç­–ç•¥å·²å¯ç”¨:")
        logger.info("  - æ™ºèƒ½å»¶è¿Ÿï¼ˆGameJolt: 3-6s, itch.io: 2-4sï¼‰")
        logger.info("  - ç‰¹æ®Šè¯·æ±‚å¤´ï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰")
        logger.info("  - URLæ¨¡å¼æ¨æ–­ï¼ˆåº”å¯¹403é”™è¯¯ï¼‰")
        
        # æ˜¾ç¤ºç™½åå•æ¨¡å¼çŠ¶æ€
        if STRICT_WHITELIST:
            logger.info("ğŸ”’ ä¸¥æ ¼ç™½åå•æ¨¡å¼ï¼šåªæ¥å—é¢„å®šä¹‰çš„å¯ä¿¡åŸŸå")
        else:
            logger.info("ğŸ¤– æ™ºèƒ½éªŒè¯æ¨¡å¼ï¼šç™½åå•ä¼˜å…ˆ + AIè¯„åˆ†ç³»ç»Ÿ")
            logger.info(f"  - ç™½åå•åŸŸå: {len(EMBEDDABLE_DOMAINS)} ä¸ª")
            logger.info("  - æ™ºèƒ½è¯„åˆ†: åŸºäºåŸŸåã€è·¯å¾„ã€æ–‡ä»¶åç­‰ç‰¹å¾")
        
        # åˆå§‹åŒ–è¯·æ±‚å»¶è¿Ÿè·Ÿè¸ª
        self.last_request_time = {}
        self.domain_request_count = {}
        
        # è·Ÿè¸ª429é”™è¯¯çš„åŸŸåï¼ˆç”¨äºå¢åŠ å»¶è¿Ÿï¼‰
        self.rate_limited_domains = set()
    
    @retry(stop=stop_after_attempt(3), 
           wait=wait_fixed(2),
           retry=retry_if_exception_type((RequestException, ConnectionError, Timeout)))
    def _make_request(self, url, method='get', **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚ï¼ˆä½¿ç”¨å…¨å±€ä»£ç†ï¼‰"""
        headers = kwargs.pop('headers', get_random_headers())
        
        # æ™ºèƒ½å»¶è¿Ÿç­–ç•¥
        self._apply_smart_delay(url)
        
        # å…¨å±€ä»£ç†å·²é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®š
        # requestsä¼šè‡ªåŠ¨ä½¿ç”¨HTTP_PROXY/HTTPS_PROXYç¯å¢ƒå˜é‡
        
        try:
            if method.lower() == 'head':
                response = self.session.head(url, headers=headers, timeout=10, **kwargs)
            else:
                response = self.session.get(url, headers=headers, timeout=15, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.HTTPError as e:
            # ç‰¹æ®Šå¤„ç†429é”™è¯¯ï¼ˆé¢‘ç‡é™åˆ¶ï¼‰
            if hasattr(e.response, 'status_code') and e.response.status_code == 429:
                domain = urlparse(url).netloc
                self.rate_limited_domains.add(domain)
                logger.error(f"ğŸš« 429é”™è¯¯ï¼{domain} è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œå·²æ ‡è®°ä¸ºé«˜é£é™©åŸŸå")
                # ç«‹å³ç­‰å¾…ä¸€æ®µæ—¶é—´
                logger.info("â³ ç«‹å³ä¼‘æ¯30ç§’ä»¥é¿å…ç»§ç»­è§¦å‘é™åˆ¶...")
                time.sleep(30)
            # ç‰¹æ®Šå¤„ç†403é”™è¯¯ï¼ˆå†…å®¹ä¿æŠ¤ï¼‰
            elif hasattr(e.response, 'status_code') and e.response.status_code == 403:
                domain = urlparse(url).netloc
                logger.debug(f"ğŸ›¡ï¸ {domain} 403é”™è¯¯ï¼ˆå†…å®¹ä¿æŠ¤ï¼‰ï¼Œå°è¯•ä½¿ç”¨æ¨æ–­URL")
            logger.warning(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            raise
        except Exception as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            raise
    
    def _apply_smart_delay(self, url: str):
        """æ™ºèƒ½å»¶è¿Ÿç­–ç•¥ï¼Œæ ¹æ®åŸŸåå’Œè¯·æ±‚é¢‘ç‡è°ƒæ•´"""
        parsed = urlparse(url)
        domain = parsed.netloc
        current_time = time.time()
        
        # è·Ÿè¸ªæ¯ä¸ªåŸŸåçš„è¯·æ±‚æ¬¡æ•°
        if domain not in self.domain_request_count:
            self.domain_request_count[domain] = 0
        self.domain_request_count[domain] += 1
        
        # æ ¹æ®åŸŸåä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å»¶è¿Ÿç­–ç•¥
        platform = 'default'
        for platform_name in Config.PLATFORM_DELAYS.keys():
            if platform_name != 'default' and platform_name in domain:
                platform = platform_name
                break
        
        min_delay, max_delay = Config.PLATFORM_DELAYS[platform]
        logger.debug(f"ğŸš¦ [{platform}] ä½¿ç”¨å»¶è¿Ÿ: {min_delay}-{max_delay}s")
        
        # å¦‚æœè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œå¢åŠ å»¶è¿Ÿ
        request_count = self.domain_request_count[domain]
        if request_count > 5:
            min_delay *= 1.5
            max_delay *= 1.5
            logger.warning(f"âš ï¸ [{domain}] è¯·æ±‚é¢‘ç¹ï¼Œå»¶è¿Ÿå¢åŠ åˆ° {min_delay:.1f}-{max_delay:.1f}s")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰429é”™è¯¯å†å²ï¼Œå¦‚æœæœ‰åˆ™å¤§å¹…å¢åŠ å»¶è¿Ÿ
        if hasattr(self, 'rate_limited_domains') and domain in self.rate_limited_domains:
            min_delay *= 2.0
            max_delay *= 2.0
            logger.warning(f"ğŸš« [{domain}] æ£€æµ‹åˆ°429é”™è¯¯å†å²ï¼Œå»¶è¿Ÿå¢åŠ åˆ° {min_delay:.1f}-{max_delay:.1f}s")
        
        # ç¡®ä¿ä¸ä¸Šæ¬¡è¯·æ±‚çš„æ—¶é—´é—´éš”
        if domain in self.last_request_time:
            elapsed = current_time - self.last_request_time[domain]
            if elapsed < min_delay:
                sleep_time = min_delay - elapsed + random.uniform(0, 1)
                time.sleep(sleep_time)
        
        # è®°å½•è¯·æ±‚æ—¶é—´
        self.last_request_time[domain] = time.time()
        
        # åŸºç¡€éšæœºå»¶è¿Ÿ
        time.sleep(random.uniform(min_delay, max_delay))
    
    def read_games_file(self) -> List[Dict]:
        """è¯»å–å½“å‰games.tsæ–‡ä»¶ä¸­çš„æ¸¸æˆæ•°æ®"""
        try:
            with open(GAMES_DATA_FILE, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–gamesæ•°ç»„
            start_marker = 'export const games: Game[] = ['
            end_marker = '];'
            
            start_idx = content.find(start_marker)
            if start_idx == -1:
                logger.error("æ‰¾ä¸åˆ°gamesæ•°ç»„å¼€å§‹æ ‡è®°")
                return []
            
            start_idx += len(start_marker)
            end_idx = content.find(end_marker, start_idx)
            if end_idx == -1:
                logger.error("æ‰¾ä¸åˆ°gamesæ•°ç»„ç»“æŸæ ‡è®°")
                return []
            
            games_str = content[start_idx:end_idx].strip()
            
            # ç®€å•è§£ææ¸¸æˆå¯¹è±¡
            games = []
            game_objects = self._extract_game_objects(games_str)
            
            for game_obj in game_objects:
                game_data = self._parse_game_object(game_obj)
                if game_data:
                    games.append(game_data)
            
            logger.info(f"æˆåŠŸè¯»å– {len(games)} ä¸ªæ¸¸æˆ")
            return games
            
        except Exception as e:
            logger.error(f"è¯»å–games.tsæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _extract_game_objects(self, games_str: str) -> List[str]:
        """æå–æ¸¸æˆå¯¹è±¡å­—ç¬¦ä¸²"""
        objects = []
        depth = 0
        current_obj = ""
        
        for char in games_str:
            if char == '{':
                if depth == 0:
                    current_obj = "{"
                else:
                    current_obj += char
                depth += 1
            elif char == '}':
                depth -= 1
                current_obj += char
                if depth == 0:
                    objects.append(current_obj.strip())
                    current_obj = ""
            elif depth > 0:
                current_obj += char
        
        return objects
    
    def _parse_game_object(self, obj_str: str) -> Optional[Dict]:
        """è§£æå•ä¸ªæ¸¸æˆå¯¹è±¡"""
        try:
            game = {}
            
            # æå–å„ä¸ªå­—æ®µ
            patterns = {
                'id': r"id:\s*['\"]([^'\"]+)['\"]",
                'title': r"title:\s*['\"]([^'\"]+)['\"]",
                'description': r"description:\s*['\"]([^'\"]+)['\"]",
                'category': r"category:\s*['\"]([^'\"]+)['\"]",
                'categoryId': r"categoryId:\s*['\"]([^'\"]+)['\"]",
                'thumbnail': r"thumbnail:\s*['\"]([^'\"]+)['\"]",
                'path': r"path:\s*['\"]([^'\"]+)['\"]",
                'featured': r"featured:\s*(true|false)",
                'type': r"type:\s*['\"]([^'\"]+)['\"]",
                'iframeUrl': r"iframeUrl:\s*['\"]([^'\"]+)['\"]",
                'staticPath': r"staticPath:\s*['\"]([^'\"]+)['\"]",
                'addedAt': r"addedAt:\s*['\"]([^'\"]+)['\"]"
            }
            
            for field, pattern in patterns.items():
                match = re.search(pattern, obj_str)
                if match:
                    value = match.group(1)
                    if field == 'featured':
                        game[field] = value == 'true'
                    else:
                        game[field] = value
            
            # æå–tagsæ•°ç»„
            tags_match = re.search(r'tags:\s*\[(.*?)\]', obj_str, re.DOTALL)
            if tags_match:
                tags_str = tags_match.group(1)
                tags = [tag.strip(' "\'') for tag in tags_str.split(',') if tag.strip(' "\'')]
                game['tags'] = tags
            
            return game if 'id' in game and 'title' in game else None
            
        except Exception as e:
            logger.warning(f"è§£ææ¸¸æˆå¯¹è±¡å¤±è´¥: {e}")
            return None
    
    def clean_games(self) -> List[Dict]:
        """æ¸…ç†æ¸¸æˆæ•°æ®ï¼Œç§»é™¤æ— æ•ˆæ¸¸æˆ"""
        games = self.read_games_file()
        valid_games = []
        
        for game in games:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not all(key in game for key in ['id', 'title', 'type']):
                logger.warning(f"æ¸¸æˆç¼ºå°‘å¿…éœ€å­—æ®µ: {game.get('title', 'Unknown')}")
                continue
            
            # æ£€æŸ¥æ ‡é¢˜æœ‰æ•ˆæ€§
            title = game['title'].strip()
            if len(title) < 2 or len(title) > 100:
                logger.warning(f"æ¸¸æˆæ ‡é¢˜æ— æ•ˆ: {title}")
                continue
            
            # æ£€æŸ¥ç±»å‹å’ŒURL
            if game['type'] == 'iframe':
                if 'iframeUrl' not in game:
                    logger.warning(f"iframeæ¸¸æˆç¼ºå°‘URL: {title}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¯åµŒå…¥çš„åŸŸå
                parsed = urlparse(game['iframeUrl'])
                if not any(domain in parsed.netloc for domain in EMBEDDABLE_DOMAINS):
                    logger.warning(f"åŸŸåä¸åœ¨ç™½åå•ä¸­: {parsed.netloc} - {title}")
                    continue
            
            elif game['type'] == 'static':
                if 'staticPath' not in game:
                    logger.warning(f"é™æ€æ¸¸æˆç¼ºå°‘è·¯å¾„: {title}")
                    continue
            
            valid_games.append(game)
        
        logger.info(f"æ¸…ç†å®Œæˆ: {len(valid_games)}/{len(games)} ä¸ªæœ‰æ•ˆæ¸¸æˆ")
        return valid_games
    
    def remove_duplicates(self, games: List[Dict]) -> List[Dict]:
        """ç§»é™¤é‡å¤æ¸¸æˆ"""
        seen_titles = set()
        seen_urls = set()
        unique_games = []
        
        for game in games:
            title = game['title'].lower().strip()
            
            # æ£€æŸ¥æ ‡é¢˜é‡å¤
            if title in seen_titles:
                logger.info(f"ç§»é™¤é‡å¤æ ‡é¢˜: {game['title']}")
                continue
            
            # æ£€æŸ¥URLé‡å¤
            url_key = game.get('iframeUrl') or game.get('staticPath', '')
            if url_key and url_key in seen_urls:
                logger.info(f"ç§»é™¤é‡å¤URL: {game['title']}")
                continue
            
            seen_titles.add(title)
            if url_key:
                seen_urls.add(url_key)
            
            unique_games.append(game)
        
        logger.info(f"å»é‡å®Œæˆ: {len(unique_games)}/{len(games)} ä¸ªå”¯ä¸€æ¸¸æˆ")
        return unique_games
    
    def fix_thumbnails(self, games: List[Dict]) -> List[Dict]:
        """ä¿®å¤æ¸¸æˆå°é¢ï¼Œä¸ºæ¯ä¸ªæ¸¸æˆåˆ†é…åˆé€‚çš„ç¼©ç•¥å›¾"""
        try:
            # è·å–å¯ç”¨çš„ç¼©ç•¥å›¾æ–‡ä»¶
            available_thumbs = []
            for file in os.listdir(THUMBNAILS_DIR):
                if file.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')):
                    available_thumbs.append(file)
            
            # æ’é™¤default.jpg
            available_thumbs = [thumb for thumb in available_thumbs if thumb != 'default.jpg']
            available_thumbs.sort()  # æŒ‰æ–‡ä»¶åæ’åº
            
            logger.info(f"æ‰¾åˆ° {len(available_thumbs)} ä¸ªå¯ç”¨ç¼©ç•¥å›¾")
            
            if not available_thumbs:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç¼©ç•¥å›¾æ–‡ä»¶")
                return games
            
            # ä¸ºæ¯ä¸ªæ¸¸æˆåˆ†é…ç¼©ç•¥å›¾
            for i, game in enumerate(games):
                if available_thumbs:
                    # å¾ªç¯ä½¿ç”¨å¯ç”¨çš„ç¼©ç•¥å›¾
                    thumb_index = i % len(available_thumbs)
                    thumbnail_file = available_thumbs[thumb_index]
                    game['thumbnail'] = f'/games/thumbnails/{thumbnail_file}'
                    logger.info(f"ä¸ºæ¸¸æˆ '{game['title']}' åˆ†é…ç¼©ç•¥å›¾: {thumbnail_file}")
                else:
                    game['thumbnail'] = '/games/thumbnails/default.jpg'
            
            return games
            
        except Exception as e:
            logger.error(f"ä¿®å¤ç¼©ç•¥å›¾å¤±è´¥: {e}")
            return games
    
    def write_games_file(self, games: List[Dict]):
        """å†™å…¥æ¸¸æˆæ•°æ®åˆ°games.tsæ–‡ä»¶"""
        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_file = f"{GAMES_DATA_FILE}.backup.{int(time.time())}"
            shutil.copy2(GAMES_DATA_FILE, backup_file)
            logger.info(f"å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
            
            # è¯»å–åŸæ–‡ä»¶å†…å®¹
            with open(GAMES_DATA_FILE, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°åˆ†ç±»è®¡æ•°
            category_counts = {}
            for game in games:
                cat_id = game.get('categoryId', '1')
                category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
            
            # æ›´æ–°categorieséƒ¨åˆ†
            categories_pattern = r'export const categories: Category\[\] = \[(.*?)\];'
            categories_match = re.search(categories_pattern, content, re.DOTALL)
            
            if categories_match:
                categories_content = categories_match.group(1)
                
                # æ›´æ–°æ¯ä¸ªåˆ†ç±»çš„count
                def update_count(match):
                    cat_data = match.group(0)
                    id_match = re.search(r"id:\s*['\"](\d+)['\"]", cat_data)
                    if id_match:
                        cat_id = id_match.group(1)
                        count = category_counts.get(cat_id, 0)
                        return re.sub(r'count:\s*\d+', f'count: {count}', cat_data)
                    return cat_data
                
                updated_categories = re.sub(r'\{[^}]+\}', update_count, categories_content)
                content = content.replace(categories_match.group(1), updated_categories)
            
            # ç”Ÿæˆæ¸¸æˆæ•°ç»„ä»£ç 
            games_code = self._generate_games_code(games)
            
            # æ›¿æ¢gamesæ•°ç»„
            start_marker = 'export const games: Game[] = ['
            end_marker = '];'
            
            start_idx = content.find(start_marker)
            end_idx = content.find(end_marker, start_idx) + len(end_marker)
            
            if start_idx != -1 and end_idx != -1:
                new_content = (
                    content[:start_idx] + 
                    start_marker + '\n' + 
                    games_code + '\n' + 
                    end_marker + 
                    content[end_idx:]
                )
                
                # å†™å…¥æ–‡ä»¶
                with open(GAMES_DATA_FILE, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                logger.info(f"âœ… æˆåŠŸæ›´æ–°games.tsæ–‡ä»¶ï¼ŒåŒ…å« {len(games)} ä¸ªæ¸¸æˆ")
            else:
                logger.error("æ— æ³•æ‰¾åˆ°gamesæ•°ç»„ä½ç½®")
                
        except Exception as e:
            logger.error(f"å†™å…¥games.tsæ–‡ä»¶å¤±è´¥: {e}")
    
    def _generate_games_code(self, games: List[Dict]) -> str:
        """ç”Ÿæˆæ¸¸æˆæ•°ç»„çš„TypeScriptä»£ç """
        lines = []
        
        for game in games:
            lines.append('  {')
            lines.append(f"    id: '{game['id']}',")
            lines.append(f"    title: '{game['title']}',")
            lines.append(f"    description: '{game.get('description', '')}',")
            lines.append(f"    category: '{game.get('category', 'ä¼‘é—²')}',")
            lines.append(f"    categoryId: '{game.get('categoryId', '1')}',")
            lines.append(f"    thumbnail: '{game.get('thumbnail', '/games/thumbnails/default.jpg')}',")
            lines.append(f"    path: '{game.get('path', f'/games/{game['id']}')}',")
            lines.append(f"    featured: {str(game.get('featured', False)).lower()},")
            lines.append(f"    type: '{game['type']}',")
            
            if game['type'] == 'iframe':
                lines.append(f"    iframeUrl: '{game['iframeUrl']}',")
            elif game['type'] == 'static':
                lines.append(f"    staticPath: '{game['staticPath']}',")
            
            lines.append(f"    addedAt: '{game.get('addedAt', datetime.now().strftime('%Y-%m-%d'))}',")
            
            tags = game.get('tags', ['ä¼‘é—²'])
            tags_str = ', '.join([f'"{tag}"' for tag in tags])
            lines.append(f"    tags: [{tags_str}]")
            lines.append('  },')
        
        return '\n'.join(lines)
    
    def crawl_new_games(self, max_games: int = 10) -> List[Dict]:
        """çˆ¬å–æ–°æ¸¸æˆï¼ˆç»„åˆä½¿ç”¨åŸºç¡€çˆ¬è™«å’ŒAPIæœç´¢ï¼‰"""
        logger.info("ğŸš€ å¼€å§‹çˆ¬å–æ–°æ¸¸æˆ...")
        all_new_games = []
        
        # 1. åŸºç¡€çˆ¬è™«ï¼ˆå¤šä¸ªå¹³å°ï¼‰
        basic_games = self._crawl_basic_sites(max_games // 2)
        all_new_games.extend(basic_games)
        
        # 2. APIæœç´¢ï¼ˆå¦‚æœé…ç½®äº†APIï¼‰
        if self.has_serpapi or self.has_google_api:
            api_games = self._crawl_with_api(max_games - len(all_new_games))
            all_new_games.extend(api_games)
        
        logger.info(f"çˆ¬å–å®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(all_new_games)} ä¸ªæ–°æ¸¸æˆ")
        return all_new_games
    
    def _crawl_basic_sites(self, max_games: int) -> List[Dict]:
        """åŸºç¡€ç«™ç‚¹çˆ¬è™«ï¼ˆæ”¯æŒå¤šä¸ªå¹³å°ï¼Œæ™ºèƒ½æ£€æµ‹é€‰æ‹©å™¨ï¼‰"""
        logger.info("ğŸŒ å¼€å§‹åŸºç¡€ç«™ç‚¹çˆ¬å–...")
        new_games = []
        
        for site in PREMIUM_GAME_SITES:
            if len(new_games) >= max_games:
                break
                
            try:
                logger.info(f"çˆ¬å–å¹³å°: {site['name']}")
                response = self._make_request(site['search_url'])
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ™ºèƒ½æ£€æµ‹é€‰æ‹©å™¨ï¼ˆå¦‚æœæœªé…ç½®çš„è¯ï¼‰
                game_selector = site.get('game_selector')
                title_selector = site.get('title_selector')
                
                if not game_selector or not title_selector:
                    logger.info(f"ğŸ” è‡ªåŠ¨æ£€æµ‹ {site['name']} çš„CSSé€‰æ‹©å™¨...")
                    detected_selectors = self._detect_game_selectors(soup, site['name'])
                    
                    if not game_selector:
                        game_selector = detected_selectors.get('game_selector')
                    if not title_selector:
                        title_selector = detected_selectors.get('title_selector')
                    
                    if game_selector and title_selector:
                        logger.info(f"âœ… æ£€æµ‹æˆåŠŸ: game='{game_selector}', title='{title_selector}'")
                    else:
                        logger.warning(f"âŒ é€‰æ‹©å™¨æ£€æµ‹å¤±è´¥ï¼Œè·³è¿‡å¹³å°: {site['name']}")
                        continue
                
                game_elements = soup.select(game_selector)[:max_games - len(new_games)]
                logger.info(f"æ‰¾åˆ° {len(game_elements)} ä¸ªæ¸¸æˆå…ƒç´ ")
                
                for i, element in enumerate(game_elements):
                    try:
                        title_elem = element.select_one(title_selector)
                        if not title_elem:
                            # å°è¯•å¤‡ç”¨æ ‡é¢˜é€‰æ‹©å™¨
                            title_elem = self._find_title_element(element)
                        
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        if len(title) < 3:
                            continue
                        
                        link_elem = element.select_one('a[href]')
                        if not link_elem:
                            # å¦‚æœå…ƒç´ æœ¬èº«å°±æ˜¯é“¾æ¥
                            if element.name == 'a' and element.get('href'):
                                link_elem = element
                            else:
                                continue
                        
                        game_url = urljoin(site['base_url'], link_elem['href'])
                        
                        # å°è¯•æŸ¥æ‰¾iframe URL
                        iframe_url = self._find_iframe_url(game_url)
                        if iframe_url and self._verify_iframe_playable(iframe_url):
                            game_id = f"basic_{site['name'].lower().replace(' ', '_')}_{int(time.time())}_{i}"
                            
                            game_info = {
                                'id': game_id,
                                'title': title,
                                'description': f"æ¥è‡ª{site['name']}çš„HTML5æ¸¸æˆ",
                                'category': 'ä¼‘é—²',
                                'categoryId': '1',
                                'thumbnail': '/games/thumbnails/default.jpg',
                                'path': f'/games/{game_id}',
                                'featured': False,
                                'type': 'iframe',
                                'iframeUrl': iframe_url,
                                'addedAt': datetime.now().strftime('%Y-%m-%d'),
                                'tags': ['HTML5', 'åœ¨çº¿', site['name']]
                            }
                            new_games.append(game_info)
                            logger.info(f"âœ… åŸºç¡€çˆ¬å–æ‰¾åˆ°æ¸¸æˆ: {title} - {site['name']}")
                        
                        time.sleep(random.uniform(2, 4))
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†æ¸¸æˆå¤±è´¥: {e}")
                        continue
                
            except Exception as e:
                logger.error(f"å¹³å° {site['name']} çˆ¬å–å¤±è´¥: {e}")
                continue
        
        logger.info(f"åŸºç¡€çˆ¬å–å®Œæˆï¼Œæ‰¾åˆ° {len(new_games)} ä¸ªæ¸¸æˆ")
        return new_games
    
    def _detect_game_selectors(self, soup: BeautifulSoup, site_name: str) -> Dict[str, str]:
        """æ™ºèƒ½æ£€æµ‹æ¸¸æˆç›¸å…³çš„CSSé€‰æ‹©å™¨"""
        # å¸¸è§çš„æ¸¸æˆå®¹å™¨é€‰æ‹©å™¨æ¨¡å¼
        game_container_patterns = [
            # å¸¸è§çš„classåç§°æ¨¡å¼
            '.game', '.game-item', '.game-card', '.game-cell', '.game-tile',
            '.game-thumbnail', '.game-box', '.game-entry', '.game-listing',
            '[class*="game"]', '[class*="item"]', '[class*="card"]', 
            '[class*="thumb"]', '[class*="entry"]',
            
            # IDæ¨¡å¼
            '[id*="game"]', '[id*="item"]',
            
            # é€šç”¨å®¹å™¨
            '.item', '.card', '.entry', '.box', '.tile', '.cell',
            '.thumbnail', '.thumb', '.listing', '.product'
        ]
        
        # å¸¸è§çš„æ ‡é¢˜é€‰æ‹©å™¨æ¨¡å¼
        title_patterns = [
            '.title', '.name', '.game-title', '.game-name', '.item-title',
            '.card-title', '.entry-title', '.product-title',
            'h1', 'h2', 'h3', 'h4', '.heading',
            '[class*="title"]', '[class*="name"]', '[class*="heading"]',
            'a', '.link'
        ]
        
        best_game_selector = None
        best_title_selector = None
        best_score = 0
        
        logger.debug(f"ğŸ” åˆ†æ {site_name} é¡µé¢ç»“æ„...")
        
        # æµ‹è¯•æ¸¸æˆå®¹å™¨é€‰æ‹©å™¨
        for game_pattern in game_container_patterns:
            try:
                game_elements = soup.select(game_pattern)
                
                # è¿‡æ»¤æ‰æ˜æ˜¾ä¸åˆç†çš„ç»“æœ
                if len(game_elements) < 3 or len(game_elements) > 100:
                    continue
                
                # æµ‹è¯•æ ‡é¢˜é€‰æ‹©å™¨
                for title_pattern in title_patterns:
                    score = self._evaluate_selector_combination(
                        game_elements, title_pattern, site_name)
                    
                    if score > best_score:
                        best_score = score
                        best_game_selector = game_pattern
                        best_title_selector = title_pattern
                        
            except Exception as e:
                logger.debug(f"é€‰æ‹©å™¨æµ‹è¯•å¤±è´¥ {game_pattern}: {e}")
                continue
        
        result = {}
        if best_game_selector and best_title_selector and best_score > 5:
            result['game_selector'] = best_game_selector
            result['title_selector'] = best_title_selector
            logger.info(f"ğŸ¯ æœ€ä½³é€‰æ‹©å™¨ç»„åˆ (å¾—åˆ†: {best_score})")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„é€‰æ‹©å™¨ç»„åˆ (æœ€é«˜å¾—åˆ†: {best_score})")
        
        return result
    
    def _evaluate_selector_combination(self, game_elements: list, title_pattern: str, site_name: str) -> int:
        """è¯„ä¼°é€‰æ‹©å™¨ç»„åˆçš„è´¨é‡"""
        score = 0
        valid_titles = 0
        has_links = 0
        
        # å–æ ·æµ‹è¯•ï¼ˆæœ€å¤šæµ‹è¯•å‰10ä¸ªå…ƒç´ ï¼‰
        sample_elements = game_elements[:min(10, len(game_elements))]
        
        for element in sample_elements:
            try:
                # æµ‹è¯•æ ‡é¢˜é€‰æ‹©å™¨
                title_elem = element.select_one(title_pattern)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    
                    # éªŒè¯æ ‡é¢˜è´¨é‡
                    if self._is_valid_game_title(title_text):
                        valid_titles += 1
                        score += 2
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥
                link_elem = element.select_one('a[href]')
                if link_elem or (element.name == 'a' and element.get('href')):
                    has_links += 1
                    score += 1
                
                # æ£€æŸ¥å…ƒç´ ç»“æ„åˆç†æ€§
                if self._has_reasonable_structure(element):
                    score += 1
                    
            except Exception:
                continue
        
        # è®¡ç®—è´¨é‡æ¯”ä¾‹
        if len(sample_elements) > 0:
            title_ratio = valid_titles / len(sample_elements)
            link_ratio = has_links / len(sample_elements)
            
            # æ ‡é¢˜è¦†ç›–ç‡æƒé‡
            score += int(title_ratio * 10)
            # é“¾æ¥è¦†ç›–ç‡æƒé‡
            score += int(link_ratio * 5)
            
            # æ•°é‡åˆç†æ€§å¥–åŠ±
            total_elements = len(game_elements)
            if 5 <= total_elements <= 50:
                score += 5
            elif 3 <= total_elements <= 100:
                score += 3
        
        return score
    
    def _is_valid_game_title(self, title: str) -> bool:
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ¸¸æˆæ ‡é¢˜"""
        if not title or len(title.strip()) < 2:
            return False
        
        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ¸¸æˆçš„æ ‡é¢˜
        invalid_keywords = [
            'menu', 'navigation', 'header', 'footer', 'sidebar',
            'advertisement', 'ad', 'sponsor', 'login', 'register',
            'search', 'filter', 'sort', 'category', 'tag',
            'more', 'view all', 'load more', 'next', 'previous',
            'home', 'about', 'contact', 'privacy', 'terms'
        ]
        
        title_lower = title.lower()
        if any(keyword in title_lower for keyword in invalid_keywords):
            return False
        
        # æ£€æŸ¥é•¿åº¦åˆç†æ€§
        if len(title) > 100:
            return False
        
        # åŒ…å«æ¸¸æˆç›¸å…³è¯æ±‡åŠ åˆ†
        game_keywords = ['game', 'play', 'adventure', 'puzzle', 'action', 'fun']
        if any(keyword in title_lower for keyword in game_keywords):
            return True
        
        # åŸºæœ¬éªŒè¯ï¼šåŒ…å«å­—æ¯å’Œåˆç†é•¿åº¦
        return len(title.strip()) >= 3 and any(c.isalpha() for c in title)
    
    def _has_reasonable_structure(self, element) -> bool:
        """æ£€æŸ¥å…ƒç´ æ˜¯å¦æœ‰åˆç†çš„æ¸¸æˆæ¡ç›®ç»“æ„"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„æ¸¸æˆä¿¡æ¯å…ƒç´ 
        has_image = bool(element.select('img'))
        has_link = bool(element.select('a[href]')) or (element.name == 'a' and element.get('href'))
        has_text_content = bool(element.get_text(strip=True))
        
        # è‡³å°‘è¦æœ‰é“¾æ¥å’Œæ–‡æœ¬å†…å®¹
        return has_link and has_text_content
    
    def _find_title_element(self, element):
        """åœ¨å…ƒç´ ä¸­æŸ¥æ‰¾æœ€å¯èƒ½çš„æ ‡é¢˜å…ƒç´ """
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [
            '.title', '.name', '.game-title', '.game-name',
            'h1', 'h2', 'h3', 'h4',
            '[class*="title"]', '[class*="name"]',
            'a[href]'  # é“¾æ¥æ–‡æœ¬é€šå¸¸æ˜¯æ ‡é¢˜
        ]
        
        for selector in title_selectors:
            title_elem = element.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if self._is_valid_game_title(title_text):
                    return title_elem
        
        return None
    
    def _crawl_with_api(self, max_games: int) -> List[Dict]:
        """ä½¿ç”¨APIæœç´¢æ¸¸æˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        logger.info("ğŸ” å¼€å§‹APIæœç´¢...")
        api_games = []
        
        for query in GAME_SEARCH_QUERIES[:3]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡
            if len(api_games) >= max_games:
                break
                
            logger.info(f"æœç´¢æŸ¥è¯¢: {query}")
            
            # ä¼˜å…ˆä½¿ç”¨SerpAPI
            if self.has_serpapi:
                serp_results = self._search_with_serpapi_enhanced(query, max_games - len(api_games))
                api_games.extend(serp_results)
                time.sleep(random.uniform(3, 5))  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # å¤‡ç”¨Google Custom Search
            elif self.has_google_api:
                google_results = self._search_with_google(query, max_games - len(api_games))
                api_games.extend(google_results)
                time.sleep(random.uniform(3, 5))
        
        logger.info(f"APIæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(api_games)} ä¸ªæ¸¸æˆ")
        return api_games
    
    @retry(stop=stop_after_attempt(3), wait=wait_fixed(3))
    def _search_with_serpapi_enhanced(self, query: str, max_results: int) -> List[Dict]:
        """ä½¿ç”¨SerpAPIæœç´¢ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        results = []
        
        try:
            if SERPAPI_AVAILABLE:
                # ä½¿ç”¨æ­£ç¡®çš„SerpAPIè°ƒç”¨æ–¹å¼
                params = {
                    'q': query,
                    'api_key': SERPAPI_KEY,
                    'engine': 'google',
                    'num': min(max_results, 10),
                    'hl': 'en',
                    'gl': 'us'
                }
                
                # ä½¿ç”¨æ­£ç¡®çš„searchæ–¹æ³•ï¼Œä¼ å…¥å­—å…¸å‚æ•°
                search_results = serpapi.search(params)
                # SerpResultså¯¹è±¡éœ€è¦è½¬æ¢ä¸ºå­—å…¸
                data = search_results.as_dict() if hasattr(search_results, 'as_dict') else search_results
            
            else:
                # å¤‡ç”¨HTTPè°ƒç”¨
                params = {
                    'q': query,
                    'api_key': SERPAPI_KEY,
                    'engine': 'google',
                    'num': min(max_results, 10),
                    'hl': 'en'
                }
                
                response = self._make_request('https://serpapi.com/search', params=params)
                data = response.json()
            
            for i, result in enumerate(data.get('organic_results', [])[:max_results]):
                try:
                    title = result.get('title', '').strip()
                    link = result.get('link', '')
                    snippet = result.get('snippet', '')
                    
                    if not title or not link or len(title) < 3:
                        continue
                    
                    # éªŒè¯æ˜¯å¦æ˜¯æ¸¸æˆç›¸å…³
                    if not self._is_game_related(title, snippet):
                        logger.debug(f"âŒ SerpAPIè·³è¿‡éæ¸¸æˆå†…å®¹: {title}")
                        continue
                    
                    # å°è¯•æŸ¥æ‰¾iframe URL
                    iframe_url = self._find_iframe_url(link)
                    if not iframe_url:
                        logger.debug(f"âŒ SerpAPIæœªæ‰¾åˆ°iframe: {title}")
                        continue
                    
                    if not self._verify_iframe_playable(iframe_url):
                        logger.debug(f"âŒ SerpAPI iframeä¸å¯ç”¨: {title}")
                        continue
                    
                    game_id = f"serp_{int(time.time())}_{i}"
                    
                    game_info = {
                        'id': game_id,
                        'title': self._clean_title(title),
                        'description': f"é€šè¿‡SerpAPIå‘ç°çš„HTML5æ¸¸æˆ: {snippet[:100]}...",
                        'category': self._categorize_game(title, snippet),
                        'categoryId': self._get_category_id(title, snippet),
                        'thumbnail': '/games/thumbnails/default.jpg',
                        'path': f'/games/{game_id}',
                        'featured': False,
                        'type': 'iframe',
                        'iframeUrl': iframe_url,
                        'addedAt': datetime.now().strftime('%Y-%m-%d'),
                        'tags': ['APIæœç´¢', 'HTML5', 'SerpAPI']
                    }
                    results.append(game_info)
                    logger.info(f"âœ… SerpAPIæ‰¾åˆ°æ¸¸æˆ: {title}")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†SerpAPIç»“æœå¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"SerpAPIæœç´¢å¤±è´¥: {e}")
            raise
        
        return results
    
    @retry(stop=stop_after_attempt(3), wait=wait_fixed(3))
    def _search_with_google(self, query: str, max_results: int) -> List[Dict]:
        """ä½¿ç”¨Google Custom Search APIæœç´¢"""
        results = []
        
        try:
            params = {
                'key': GOOGLE_API_KEY,
                'cx': GOOGLE_CX,
                'q': query,
                'num': min(max_results, 10)
            }
            
            response = self._make_request('https://www.googleapis.com/customsearch/v1', params=params)
            data = response.json()
            
            for i, item in enumerate(data.get('items', [])[:max_results]):
                try:
                    title = item.get('title', '').strip()
                    link = item.get('link', '')
                    snippet = item.get('snippet', '')
                    
                    if not title or not link or len(title) < 3:
                        continue
                    
                    # éªŒè¯æ˜¯å¦æ˜¯æ¸¸æˆç›¸å…³
                    if not self._is_game_related(title, snippet):
                        continue
                    
                    # å°è¯•æŸ¥æ‰¾iframe URL
                    iframe_url = self._find_iframe_url(link)
                    if iframe_url and self._verify_iframe_playable(iframe_url):
                        game_id = f"google_{int(time.time())}_{i}"
                        
                        game_info = {
                            'id': game_id,
                            'title': self._clean_title(title),
                            'description': f"é€šè¿‡Google Custom Searchå‘ç°çš„HTML5æ¸¸æˆ: {snippet[:100]}...",
                            'category': self._categorize_game(title, snippet),
                            'categoryId': self._get_category_id(title, snippet),
                            'thumbnail': '/games/thumbnails/default.jpg',
                            'path': f'/games/{game_id}',
                            'featured': False,
                            'type': 'iframe',
                            'iframeUrl': iframe_url,
                            'addedAt': datetime.now().strftime('%Y-%m-%d'),
                            'tags': ['APIæœç´¢', 'HTML5', 'Google']
                        }
                        results.append(game_info)
                        logger.info(f"âœ… Google APIæ‰¾åˆ°æ¸¸æˆ: {title}")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†Google APIç»“æœå¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"Google Custom Searchå¤±è´¥: {e}")
            raise
        
        return results
    
    def _is_game_related(self, title: str, snippet: str) -> bool:
        """éªŒè¯æ ‡é¢˜å’Œæ‘˜è¦æ˜¯å¦ä¸æ¸¸æˆç›¸å…³ä¸”å¯åœ¨çº¿ç©"""
        text = (title + ' ' + snippet).lower()
        
        # å¿…é¡»åŒ…å«çš„æ¸¸æˆå…³é”®è¯
        game_keywords = [
            'game', 'play', 'html5', 'browser', 'online',
            'arcade', 'puzzle', 'action', 'adventure', 'strategy', 'casual',
            'æ¸¸æˆ', 'ç©', 'åœ¨çº¿'
        ]
        
        # å¯åœ¨çº¿ç©çš„å…³é”®è¯
        playable_keywords = [
            'play now', 'play online', 'in browser', 'play free',
            'no download', 'instant play', 'web game', 'browser game',
            'click to play', 'start playing', 'play instantly'
        ]
        
        # æ’é™¤çš„å…³é”®è¯ï¼ˆè®ºå›ã€ä¸‹è½½ã€æ–°é—»ç­‰ï¼‰
        exclude_keywords = [
            'forum', 'discussion', 'review', 'news', 'blog', 'tutorial',
            'guide', 'download', 'wiki', 'community', 'devlog', 'discord',
            'reddit', 'youtube', 'steam', 'app store', 'google play',
            'walkthrough', 'cheats', 'tips', 'trailer', 'preview'
        ]
        
        # å¿…é¡»åŒ…å«æ¸¸æˆå…³é”®è¯
        has_game_keyword = any(keyword in text for keyword in game_keywords)
        
        # æœ€å¥½åŒ…å«å¯ç©å…³é”®è¯
        has_playable_keyword = any(keyword in text for keyword in playable_keywords)
        
        # ä¸èƒ½åŒ…å«æ’é™¤å…³é”®è¯
        has_exclude_keyword = any(keyword in text for keyword in exclude_keywords)
        
        # åˆ¤æ–­é€»è¾‘ï¼šæœ‰æ¸¸æˆå…³é”®è¯ï¼Œæ²¡æœ‰æ’é™¤å…³é”®è¯ï¼Œæœ€å¥½æœ‰å¯ç©å…³é”®è¯
        return has_game_keyword and not has_exclude_keyword and (has_playable_keyword or 'itch.io' in text or 'gamejolt' in text)
    
    def _clean_title(self, title: str) -> str:
        """æ¸…ç†æ¸¸æˆæ ‡é¢˜"""
        # ç§»é™¤å¸¸è§çš„åç¼€
        suffixes = [' - Play Online', ' | Free Game', ' - Browser Game', ' Online']
        for suffix in suffixes:
            if title.endswith(suffix):
                title = title[:-len(suffix)]
        
        return title.strip()
    
    def _categorize_game(self, title: str, snippet: str) -> str:
        """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨åˆ†ç±»æ¸¸æˆ"""
        text = (title + ' ' + snippet).lower()
        
        if any(word in text for word in ['puzzle', 'brain', 'logic', 'match', 'ç›Šæ™º', 'è°œé¢˜']):
            return 'ç›Šæ™º'
        elif any(word in text for word in ['action', 'shoot', 'fight', 'run', 'åŠ¨ä½œ', 'å°„å‡»']):
            return 'åŠ¨ä½œ'
        elif any(word in text for word in ['card', 'poker', 'solitaire', 'å¡ç‰Œ', 'çº¸ç‰Œ']):
            return 'å¡ç‰Œ'
        elif any(word in text for word in ['sport', 'football', 'soccer', 'basketball', 'ä½“è‚²', 'è¶³çƒ']):
            return 'ä½“è‚²'
        elif any(word in text for word in ['board', 'chess', 'checkers', 'æ£‹ç›˜', 'è±¡æ£‹']):
            return 'æ£‹ç›˜'
        else:
            return 'ä¼‘é—²'
    
    def _get_category_id(self, title: str, snippet: str) -> str:
        """è·å–åˆ†ç±»ID"""
        category = self._categorize_game(title, snippet)
        category_map = {
            'ä¼‘é—²': '1',
            'ç›Šæ™º': '2',
            'åŠ¨ä½œ': '3',
            'å¡ç‰Œ': '4',
            'ä½“è‚²': '5',
            'æ£‹ç›˜': '6'
        }
        return category_map.get(category, '1')
    
    @retry(stop=stop_after_attempt(2), wait=wait_fixed(3))
    def _find_iframe_url(self, game_url: str) -> Optional[str]:
        """åœ¨æ¸¸æˆé¡µé¢ä¸­æŸ¥æ‰¾iframe URLï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            # å¯¹äºæŸäº›å¹³å°ä½¿ç”¨ç‰¹æ®Šçš„è¯·æ±‚å¤´
            special_headers = self._get_special_headers(game_url)
            response = self._make_request(game_url, headers=special_headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. ä¼˜å…ˆæŸ¥æ‰¾æ˜ç¡®çš„æ¸¸æˆiframe
            game_iframes = soup.select('iframe[src*="game"], iframe[class*="game"], iframe[id*="game"]')
            for iframe in game_iframes:
                iframe_src = iframe.get('src')
                if iframe_src and self._is_valid_game_iframe(iframe_src, game_url):
                    return urljoin(game_url, iframe_src)
            
            # 2. æŸ¥æ‰¾æ‰€æœ‰iframeï¼ŒæŒ‰å¯ä¿¡åº¦æ’åº
            all_iframes = soup.select('iframe[src]')
            valid_iframes = []
            
            for iframe in all_iframes:
                iframe_src = iframe.get('src')
                if iframe_src:
                    full_url = urljoin(game_url, iframe_src)
                    if self._is_valid_game_iframe(iframe_src, game_url):
                        # è®¡ç®—å¯ä¿¡åº¦åˆ†æ•°
                        score = self._calculate_iframe_score(iframe, full_url)
                        valid_iframes.append((full_url, score))
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œè¿”å›æœ€é«˜åˆ†çš„
            if valid_iframes:
                valid_iframes.sort(key=lambda x: x[1], reverse=True)
                return valid_iframes[0][0]
            
            # 3. æŸ¥æ‰¾å…¶ä»–æ¸¸æˆåµŒå…¥æ–¹å¼
            game_elements = soup.select(
                '[data-game-url], [data-embed-url], [data-src*="game"], '
                '[class*="embed"], [id*="embed"], [class*="player"], [id*="player"]'
            )
            
            for element in game_elements:
                for attr in ['data-game-url', 'data-embed-url', 'data-src', 'src']:
                    url = element.get(attr)
                    if url and self._is_valid_game_iframe(url, game_url):
                        return urljoin(game_url, url)
            
            # 4. ç‰¹æ®Šå¹³å°å¤„ç†
            return self._extract_platform_specific_url(soup, game_url)
            
        except Exception as e:
            logger.warning(f"æŸ¥æ‰¾iframeå¤±è´¥ {game_url}: {e}")
            # å¯¹äº403é”™è¯¯ï¼Œå°è¯•åŸºäºURLæ¨¡å¼æ¨æ–­iframe
            if "403" in str(e) or "Forbidden" in str(e):
                return self._infer_iframe_from_url(game_url)
            return None
    
    def _is_valid_game_iframe(self, iframe_src: str, base_url: str) -> bool:
        """éªŒè¯iframe URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ¸¸æˆåµŒå…¥ï¼ˆæ™ºèƒ½éªŒè¯ï¼Œå‡å°‘ç™½åå•ä¾èµ–ï¼‰"""
        if not iframe_src:
            return False
        
        # è½¬æ¢ä¸ºå®Œæ•´URL
        full_url = urljoin(base_url, iframe_src)
        parsed = urlparse(full_url)
        
        # ğŸ¥‡ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šç™½åå•åŸŸåï¼ˆæœ€å¯ä¿¡ï¼‰
        is_whitelisted = any(domain in parsed.netloc or domain in full_url for domain in EMBEDDABLE_DOMAINS)
        if is_whitelisted:
            return True
        
        # å¦‚æœå¯ç”¨ä¸¥æ ¼ç™½åå•æ¨¡å¼ï¼Œåªæ¥å—ç™½åå•åŸŸå
        if STRICT_WHITELIST:
            return False
        
        # ğŸš« æ’é™¤æ˜æ˜¾ä¸æ˜¯æ¸¸æˆçš„iframe
        exclude_patterns = [
            'ads', 'analytics', 'tracking', 'social', 'comment', 'chat',
            'youtube', 'vimeo', 'twitter', 'facebook', 'instagram',
            'discord', 'reddit', 'forum', 'feedback', 'survey'
        ]
        
        for pattern in exclude_patterns:
            if pattern in full_url.lower():
                return False
        
        # ğŸ® æ™ºèƒ½æ¸¸æˆURLæ£€æµ‹ï¼ˆæ— éœ€ç™½åå•ï¼‰
        score = self._calculate_game_url_score(full_url, parsed)
        
        # å¦‚æœå¾—åˆ†è¶³å¤Ÿé«˜ï¼Œå³ä½¿ä¸åœ¨ç™½åå•ä¸­ä¹Ÿæ¥å—
        is_valid = score >= Config.GAME_URL_SCORE_THRESHOLD
        
        if is_valid:
            logger.info(f"ğŸ¤– æ™ºèƒ½éªŒè¯é€šè¿‡ (å¾—åˆ†: {score}): {full_url}")
        else:
            logger.debug(f"ğŸ¤– æ™ºèƒ½éªŒè¯å¤±è´¥ (å¾—åˆ†: {score}): {full_url}")
        
        return is_valid
    
    def _calculate_game_url_score(self, full_url: str, parsed) -> int:
        """è®¡ç®—æ¸¸æˆURLçš„å¯ä¿¡åº¦è¯„åˆ†ï¼ˆä¸ä¾èµ–ç™½åå•ï¼‰"""
        score = 0
        url_lower = full_url.lower()
        domain = parsed.netloc.lower()
        path = parsed.path.lower()
        
        # ğŸ¯ æ¸¸æˆç›¸å…³è·¯å¾„æ£€æµ‹ï¼ˆé«˜åˆ†ï¼‰
        game_paths = [
            '/game/', '/games/', '/play/', '/embed/', '/player/', '/html5/',
            '/swf/', '/flash/', '/unity/', '/webgl/', '/canvas/'
        ]
        
        for pattern in game_paths:
            if pattern in path:
                score += 25
                break  # åªåŠ ä¸€æ¬¡åˆ†
        
        # ğŸ¯ æ¸¸æˆç›¸å…³æ–‡ä»¶åæ£€æµ‹
        game_files = [
            'game.html', 'index.html', 'main.html', 'play.html',
            'game.js', 'main.js', 'app.js', 'bundle.js'
        ]
        
        for filename in game_files:
            if filename in path:
                score += 20
                break
        
        # ğŸ¯ æ¸¸æˆç›¸å…³åŸŸåç‰¹å¾
        game_domain_keywords = [
            'game', 'play', 'arcade', 'html5', 'flash', 'unity',
            'embed', 'cdn', 'assets', 'static', 'media'
        ]
        
        for keyword in game_domain_keywords:
            if keyword in domain:
                score += 15
                break
        
        # ğŸ¯ çŸ¥åæ¸¸æˆCDNå’Œæ‰˜ç®¡æœåŠ¡æ£€æµ‹
        trusted_patterns = [
            '.itch.zone', '.hwcdn.net', '.gamedistribution.com',
            '.armorgames.com', '.kongregate.com', '.newgrounds.com',
            '.crazygames.com', '.poki.com', '.y8.com',
            'cloudfront.net', 'amazonaws.com', 'github.io'
        ]
        
        for pattern in trusted_patterns:
            if pattern in domain:
                score += 30  # é«˜åˆ†ï¼Œè¿™äº›æ˜¯å¯ä¿¡çš„æ‰˜ç®¡æœåŠ¡
                break
        
        # ğŸ¯ HTTPSåè®®åŠ åˆ†
        if parsed.scheme == 'https':
            score += 10
        
        # ğŸ¯ åˆç†çš„ç«¯å£ï¼ˆéå¸¸è§ç«¯å£å‡åˆ†ï¼‰
        if parsed.port and parsed.port not in [80, 443, 8080, 3000]:
            score -= 10
        
        # ğŸ¯ æŸ¥è¯¢å‚æ•°ä¸­çš„æ¸¸æˆæ ‡è¯†
        if parsed.query:
            query_lower = parsed.query.lower()
            if any(param in query_lower for param in ['game', 'play', 'embed', 'id=']):
                score += 10
        
        # ğŸš« å¯ç–‘åŸŸåç‰¹å¾å‡åˆ†
        suspicious_keywords = [
            'redirect', 'proxy', 'mirror', 'fake', 'spam',
            'ad', 'ads', 'banner', 'popup'
        ]
        
        for keyword in suspicious_keywords:
            if keyword in domain:
                score -= 20
        
        # ğŸš« è¿‡é•¿çš„åŸŸåå¯èƒ½æ˜¯å¯ç–‘çš„
        if len(domain) > 50:
            score -= 15
        
        # ğŸš« åŒ…å«å¤šä¸ªè¿ç»­æ•°å­—çš„åŸŸåï¼ˆå¯èƒ½æ˜¯ä¸´æ—¶åŸŸåï¼‰
        import re
        if re.search(r'\d{4,}', domain):
            score -= 10
        
        return max(0, score)  # ç¡®ä¿åˆ†æ•°ä¸ä¸ºè´Ÿæ•°
    
    def _calculate_iframe_score(self, iframe_element, iframe_url: str) -> int:
        """è®¡ç®—iframeçš„å¯ä¿¡åº¦åˆ†æ•°"""
        score = 0
        
        # URLåŸŸåæƒé‡
        parsed = urlparse(iframe_url)
        if any(domain in parsed.netloc for domain in EMBEDDABLE_DOMAINS):
            score += 100
        
        # iframeå±æ€§æ£€æŸ¥
        iframe_class = iframe_element.get('class', [])
        iframe_id = iframe_element.get('id', '')
        
        if isinstance(iframe_class, list):
            iframe_class = ' '.join(iframe_class)
        
        attrs_text = f"{iframe_class} {iframe_id}".lower()
        
        # æ¸¸æˆç›¸å…³å±æ€§åŠ åˆ†
        if any(word in attrs_text for word in ['game', 'play', 'embed', 'player']):
            score += 50
        
        # URLè·¯å¾„åŠ åˆ†
        if any(word in iframe_url.lower() for word in ['/game/', '/play/', '/embed/']):
            score += 30
        
        # iframeå°ºå¯¸æ£€æŸ¥ï¼ˆæ¸¸æˆé€šå¸¸æœ‰åˆç†çš„å°ºå¯¸ï¼‰
        width = iframe_element.get('width', '')
        height = iframe_element.get('height', '')
        
        try:
            if width and height:
                w, h = int(width), int(height)
                if 300 <= w <= 1920 and 200 <= h <= 1080:  # åˆç†çš„æ¸¸æˆå°ºå¯¸
                    score += 20
        except:
            pass
        
        return score
    
    def _extract_platform_specific_url(self, soup, base_url: str) -> Optional[str]:
        """ç‰¹å®šå¹³å°çš„URLæå–é€»è¾‘"""
        parsed_base = urlparse(base_url)
        
        # itch.io ç‰¹æ®Šå¤„ç†
        if 'itch.io' in parsed_base.netloc:
            # æŸ¥æ‰¾itch.ioçš„æ¸¸æˆåµŒå…¥
            play_buttons = soup.select('a[href*="html"], .button[href*="html"]')
            for button in play_buttons:
                href = button.get('href')
                if href and 'html' in href:
                    return urljoin(base_url, href)
        
        # GameJolt ç‰¹æ®Šå¤„ç†
        elif 'gamejolt.com' in parsed_base.netloc:
            # æŸ¥æ‰¾GameJoltçš„æ¸¸æˆåµŒå…¥
            game_frames = soup.select('[class*="game-embed"], [id*="game-embed"]')
            for frame in game_frames:
                src = frame.get('src') or frame.get('data-src')
                if src:
                    return urljoin(base_url, src)
        
        # Newgrounds ç‰¹æ®Šå¤„ç†
        elif 'newgrounds.com' in parsed_base.netloc:
            # æŸ¥æ‰¾Newgroundsçš„æ¸¸æˆåµŒå…¥
            embed_links = soup.select('a[href*="/portal/view/"]')
            for link in embed_links:
                href = link.get('href')
                if href:
                    return urljoin(base_url, href)
        
        return None
    
    def _get_special_headers(self, url: str) -> Dict[str, str]:
        """ä¸ºç‰¹å®šç½‘ç«™è¿”å›ç‰¹æ®Šçš„è¯·æ±‚å¤´"""
        headers = get_random_headers()
        parsed = urlparse(url)
        
        # GameJolt ç‰¹æ®Šå¤„ç†
        if 'gamejolt.com' in parsed.netloc:
            headers.update({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Cookie': 'frontend=true',  # æ¨¡æ‹Ÿå‰ç«¯è®¿é—®
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Pragma': 'no-cache',
                'Cache-Control': 'no-cache'
            })
        
        # itch.io ç‰¹æ®Šå¤„ç†
        elif 'itch.io' in parsed.netloc:
            headers.update({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin'
            })
        
        # Newgrounds ç‰¹æ®Šå¤„ç†
        elif 'newgrounds.com' in parsed.netloc:
            headers.update({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Referer': 'https://www.newgrounds.com/',
                'Origin': 'https://www.newgrounds.com'
            })
        
        return headers
    
    def _infer_iframe_from_url(self, game_url: str) -> Optional[str]:
        """åŸºäºæ¸¸æˆé¡µé¢URLæ¨æ–­å¯èƒ½çš„iframe URL"""
        parsed = urlparse(game_url)
        
        try:
            # GameJolt URLæ¨¡å¼æ¨æ–­
            if 'gamejolt.com' in parsed.netloc and '/games/' in parsed.path:
                # ä» /games/game-name/id æ¨æ–­åµŒå…¥URL
                path_parts = parsed.path.strip('/').split('/')
                if len(path_parts) >= 3 and path_parts[0] == 'games':
                    game_id = path_parts[-1]
                    # GameJoltçš„åµŒå…¥URLæ¨¡å¼
                    embed_url = f"https://gamejolt.net/games/embed/{game_id}"
                    logger.info(f"æ¨æ–­GameJolt iframe: {embed_url}")
                    return embed_url
            
            # itch.io URLæ¨¡å¼æ¨æ–­
            elif 'itch.io' in parsed.netloc:
                # å¾ˆå¤šitch.ioæ¸¸æˆæœ‰æ ‡å‡†çš„HTML5åµŒå…¥æ ¼å¼
                base_url = f"{parsed.scheme}://{parsed.netloc}"
                if parsed.path:
                    # å°è¯•æ·»åŠ  /embed åç¼€
                    embed_path = parsed.path.rstrip('/') + '/embed'
                    embed_url = f"{base_url}{embed_path}"
                    logger.info(f"æ¨æ–­itch.io iframe: {embed_url}")
                    return embed_url
            
            # Newgrounds URLæ¨¡å¼æ¨æ–­
            elif 'newgrounds.com' in parsed.netloc and '/portal/view/' in parsed.path:
                # Newgroundsçš„åµŒå…¥URLé€šå¸¸å°±æ˜¯åŸURL
                logger.info(f"ä½¿ç”¨NewgroundsåŸURL: {game_url}")
                return game_url
            
        except Exception as e:
            logger.debug(f"URLæ¨¡å¼æ¨æ–­å¤±è´¥: {e}")
        
        return None
    
    def _verify_iframe_playable(self, iframe_url: str) -> bool:
        """éªŒè¯iframe URLæ˜¯å¦çœŸçš„å¯ä»¥åŠ è½½æ¸¸æˆ"""
        try:
            # å‘é€HEADè¯·æ±‚æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®
            special_headers = self._get_special_headers(iframe_url)
            response = self._make_request(iframe_url, method='head', headers=special_headers)
            
            if response.status_code != 200:
                logger.debug(f"iframeå“åº”çŠ¶æ€: {response.status_code} - {iframe_url}")
                return False
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            
            # HTMLå†…å®¹é€šå¸¸æ˜¯æ¸¸æˆé¡µé¢
            if 'text/html' in content_type:
                return True
            
            # ä¸€äº›æ¸¸æˆç›´æ¥æä¾›JavaScriptæˆ–å…¶ä»–å†…å®¹
            if any(ct in content_type for ct in ['application/javascript', 'text/javascript', 'application/json']):
                return True
            
            logger.debug(f"iframeå†…å®¹ç±»å‹: {content_type} - {iframe_url}")
            return False
            
        except Exception as e:
            error_msg = str(e).lower()
            
            # å¯¹äºæŸäº›ç‰¹å®šé”™è¯¯ï¼Œæˆ‘ä»¬ä»ç„¶è®¤ä¸ºURLå¯èƒ½æœ‰æ•ˆ
            if any(pattern in error_msg for pattern in ['403', 'forbidden', 'timeout']):
                # å¦‚æœæ˜¯ç™½åå•åŸŸåï¼Œå³ä½¿403ä¹Ÿè®¤ä¸ºå¯èƒ½æœ‰æ•ˆ
                parsed = urlparse(iframe_url)
                if any(domain in parsed.netloc or domain in iframe_url for domain in EMBEDDABLE_DOMAINS):
                    logger.info(f"ç™½åå•åŸŸå403é”™è¯¯ï¼Œä»ç„¶æ¥å—: {iframe_url}")
                    return True
            
            logger.debug(f"éªŒè¯iframeå¤±è´¥: {e} - {iframe_url}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ¸¸æˆç®¡ç†å™¨ - ç»Ÿä¸€çš„æ¸¸æˆæ•°æ®ç®¡ç†å·¥å…·')
    parser.add_argument('--action', choices=['clean', 'crawl', 'fix-thumbnails', 'all'], 
                       default='all', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--max-games', type=int, default=Config.MAX_GAMES_DEFAULT, help='çˆ¬å–çš„æœ€å¤§æ¸¸æˆæ•°é‡')
    parser.add_argument('--use-proxy', action='store_true', help='å¯ç”¨ä»£ç†æ¨¡å¼ï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ USE_PROXY=true é…ç½®ï¼‰')
    parser.add_argument('--strict-whitelist', action='store_true', help='å¯ç”¨ä¸¥æ ¼ç™½åå•æ¨¡å¼ï¼Œåªæ¥å—é¢„å®šä¹‰åŸŸå')
    
    args = parser.parse_args()
    
    # ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®
    Config.update_from_args(args)
    
    # æ›´æ–°å…¨å±€å˜é‡
    global USE_PROXY, PROXY_HOST, PROXY_PORT, STRICT_WHITELIST
    USE_PROXY = Config.USE_PROXY
    PROXY_HOST = Config.PROXY_HOST
    PROXY_PORT = Config.PROXY_PORT
    STRICT_WHITELIST = Config.STRICT_WHITELIST
    
    manager = GameManager()
    
    if args.action == 'clean':
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æ¸¸æˆæ•°æ®...")
        games = manager.clean_games()
        games = manager.remove_duplicates(games)
        manager.write_games_file(games)
        
    elif args.action == 'crawl':
        logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–æ–°æ¸¸æˆï¼ˆæœ€å¤š{args.max_games}ä¸ªï¼‰...")
        new_games = manager.crawl_new_games(args.max_games)
        existing_games = manager.read_games_file()
        all_games = existing_games + new_games
        all_games = manager.remove_duplicates(all_games)
        manager.write_games_file(all_games)
        
    elif args.action == 'fix-thumbnails':
        logger.info("ğŸ–¼ï¸ å¼€å§‹ä¿®å¤æ¸¸æˆå°é¢...")
        games = manager.read_games_file()
        games = manager.fix_thumbnails(games)
        manager.write_games_file(games)
        
    elif args.action == 'all':
        logger.info("ğŸ”„ å¼€å§‹å…¨é¢ç®¡ç†...")
        # 1. æ¸…ç†ç°æœ‰æ•°æ®
        games = manager.clean_games()
        games = manager.remove_duplicates(games)
        
        # 2. ä¿®å¤ç¼©ç•¥å›¾
        games = manager.fix_thumbnails(games)
        
        # 3. çˆ¬å–æ–°æ¸¸æˆ
        new_games = manager.crawl_new_games(args.max_games)
        all_games = games + new_games
        all_games = manager.remove_duplicates(all_games)
        
        # 4. å†æ¬¡ä¿®å¤ç¼©ç•¥å›¾
        all_games = manager.fix_thumbnails(all_games)
        
        # 5. ä¿å­˜ç»“æœ
        manager.write_games_file(all_games)
    
    logger.info("âœ… æ¸¸æˆç®¡ç†å™¨æ‰§è¡Œå®Œæˆï¼")

if __name__ == '__main__':
    main() 