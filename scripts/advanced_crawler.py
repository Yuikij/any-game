#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ¸¸æˆçˆ¬è™« - ç»“åˆAPIæœç´¢å’Œæœ¬åœ°ä¸‹è½½
ä¼˜å…ˆä¸‹è½½HTML5æ¸¸æˆæ–‡ä»¶åˆ°æœ¬åœ°ï¼Œå…¶æ¬¡è€ƒè™‘iframeåµŒå…¥
"""

import requests
from bs4 import BeautifulSoup
import json
import random
import time
import os
import zipfile
import shutil
import logging
from urllib.parse import urljoin, urlparse
from datetime import datetime
from typing import List, Dict, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('advanced_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é¡¹ç›®é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GAMES_DATA_FILE = os.path.join(PROJECT_ROOT, 'src', 'data', 'games.ts')
LOCAL_GAMES_DIR = os.path.join(PROJECT_ROOT, 'public', 'games')
THUMBNAILS_DIR = os.path.join(PROJECT_ROOT, 'public', 'games', 'thumbnails')

# APIé…ç½®ï¼ˆå¯é€‰ï¼‰
SERPAPI_KEY = os.getenv('SERPAPI_KEY', '')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', '')
GOOGLE_CX = os.getenv('GOOGLE_CX', '')

# æ¨¡æ‹Ÿæµè§ˆå™¨å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}

# é«˜è´¨é‡HTML5æ¸¸æˆå¹³å°
PREMIUM_GAME_SITES = [
    {
        'name': 'itch.io HTML5',
        'base_url': 'https://itch.io',
        'search_url': 'https://itch.io/games/html5',
        'game_selector': '.game_cell',
        'title_selector': '.title',
        'download_selector': '.download_btn, .play_btn',
        'priority': 1  # æœ€é«˜ä¼˜å…ˆçº§
    },
    {
        'name': 'GameJolt HTML5',
        'base_url': 'https://gamejolt.com',
        'search_url': 'https://gamejolt.com/games/best/html',
        'game_selector': '.game-listing-item',
        'title_selector': '.game-title',
        'download_selector': '.download-button',
        'priority': 2
    },
    {
        'name': 'OpenGameArt',
        'base_url': 'https://opengameart.org',
        'search_url': 'https://opengameart.org/art-search-advanced?keys=&field_art_type_tid%5B%5D=9',
        'game_selector': '.node',
        'title_selector': '.title',
        'download_selector': '.download',
        'priority': 3
    }
]

# å¯åµŒå…¥åŸŸåç™½åå•
EMBEDDABLE_DOMAINS = [
    'html-classic.itch.zone',
    'v6p9d9t4.ssl.hwcdn.net',
    'uploads.ungrounded.net',
    'gamejolt.net',
    'crazygames.com/embed',
    'poki.com/embed'
]

class AdvancedGameCrawler:
    """é«˜çº§æ¸¸æˆçˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.downloaded_games = []
        self.iframe_games = []
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(LOCAL_GAMES_DIR, exist_ok=True)
        os.makedirs(THUMBNAILS_DIR, exist_ok=True)
    
    def search_with_api(self, query: str, max_results: int = 10) -> List[str]:
        """ä½¿ç”¨APIæœç´¢æ¸¸æˆï¼ˆå¦‚æœé…ç½®äº†APIå¯†é’¥ï¼‰"""
        urls = []
        
        # å°è¯•ä½¿ç”¨SerpAPI
        if SERPAPI_KEY:
            try:
                from serpapi import GoogleSearch
                params = {
                    'q': query,
                    'api_key': SERPAPI_KEY,
                    'num': max_results
                }
                search = GoogleSearch(params)
                results = search.get_dict()
                api_urls = [result.get('link') for result in results.get('organic_results', [])]
                urls.extend(api_urls)
                logger.info(f"SerpAPIæ‰¾åˆ° {len(api_urls)} ä¸ªç»“æœ")
            except Exception as e:
                logger.warning(f"SerpAPIæœç´¢å¤±è´¥: {e}")
        
        # å°è¯•ä½¿ç”¨Google Custom Search API
        if GOOGLE_API_KEY and GOOGLE_CX:
            try:
                params = {
                    'key': GOOGLE_API_KEY,
                    'cx': GOOGLE_CX,
                    'q': query,
                    'num': max_results
                }
                response = self.session.get('https://www.googleapis.com/customsearch/v1', params=params)
                response.raise_for_status()
                data = response.json()
                google_urls = [item.get('link') for item in data.get('items', [])]
                urls.extend(google_urls)
                logger.info(f"Google APIæ‰¾åˆ° {len(google_urls)} ä¸ªç»“æœ")
            except Exception as e:
                logger.warning(f"Google APIæœç´¢å¤±è´¥: {e}")
        
        return list(set(urls))  # å»é‡
    
    def download_html5_game(self, game_url: str, game_title: str, game_id: str) -> Optional[Dict]:
        """ä¸‹è½½HTML5æ¸¸æˆåˆ°æœ¬åœ°"""
        try:
            logger.info(f"å°è¯•ä¸‹è½½æ¸¸æˆ: {game_title}")
            
            # è·å–æ¸¸æˆé¡µé¢
            response = self.session.get(game_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            download_links = []
            
            # æŸ¥æ‰¾ç›´æ¥çš„HTML5æ¸¸æˆæ–‡ä»¶
            for link in soup.select('a[href]'):
                href = link.get('href', '')
                if any(ext in href.lower() for ext in ['.html', '.htm', 'index.html', 'game.html']):
                    download_links.append(urljoin(game_url, href))
            
            # æŸ¥æ‰¾ZIPæ–‡ä»¶ï¼ˆå¯èƒ½åŒ…å«HTML5æ¸¸æˆï¼‰
            for link in soup.select('a[href*=".zip"], a[href*="download"]'):
                href = link.get('href', '')
                if href:
                    download_links.append(urljoin(game_url, href))
            
            # å°è¯•ä¸‹è½½æ¸¸æˆæ–‡ä»¶
            for download_url in download_links[:3]:  # é™åˆ¶å°è¯•æ¬¡æ•°
                try:
                    game_data = self._download_game_file(download_url, game_id, game_title)
                    if game_data:
                        logger.info(f"âœ… æˆåŠŸä¸‹è½½æ¸¸æˆ: {game_title}")
                        return game_data
                except Exception as e:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {download_url}: {e}")
                    continue
            
            logger.warning(f"âŒ æ— æ³•ä¸‹è½½æ¸¸æˆ: {game_title}")
            return None
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¸æˆå¤±è´¥ {game_title}: {e}")
            return None
    
    def _download_game_file(self, download_url: str, game_id: str, game_title: str) -> Optional[Dict]:
        """ä¸‹è½½å…·ä½“çš„æ¸¸æˆæ–‡ä»¶"""
        try:
            response = self.session.get(download_url, timeout=30, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('Content-Type', '').lower()
            content_length = int(response.headers.get('Content-Length', 0))
            
            # é™åˆ¶æ–‡ä»¶å¤§å°ï¼ˆ50MBï¼‰
            if content_length > 50 * 1024 * 1024:
                logger.warning(f"æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡: {download_url}")
                return None
            
            game_dir = os.path.join(LOCAL_GAMES_DIR, game_id)
            os.makedirs(game_dir, exist_ok=True)
            
            if 'zip' in content_type or download_url.endswith('.zip'):
                # å¤„ç†ZIPæ–‡ä»¶
                zip_path = os.path.join(game_dir, 'game.zip')
                with open(zip_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                # è§£å‹ZIPæ–‡ä»¶
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(game_dir)
                
                os.remove(zip_path)  # åˆ é™¤ZIPæ–‡ä»¶
                
                # æŸ¥æ‰¾ä¸»HTMLæ–‡ä»¶
                html_files = []
                for root, dirs, files in os.walk(game_dir):
                    for file in files:
                        if file.endswith(('.html', '.htm')):
                            html_files.append(os.path.join(root, file))
                
                if html_files:
                    # é€‰æ‹©æœ€å¯èƒ½çš„ä¸»æ–‡ä»¶
                    main_file = self._find_main_html(html_files)
                    relative_path = os.path.relpath(main_file, LOCAL_GAMES_DIR)
                    
                    return {
                        'type': 'static',
                        'staticPath': f'/games/{relative_path.replace(os.sep, "/")}',
                        'local_path': main_file
                    }
            
            elif 'html' in content_type or download_url.endswith(('.html', '.htm')):
                # å¤„ç†HTMLæ–‡ä»¶
                html_path = os.path.join(game_dir, 'index.html')
                with open(html_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                return {
                    'type': 'static',
                    'staticPath': f'/games/{game_id}/index.html',
                    'local_path': html_path
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _find_main_html(self, html_files: List[str]) -> str:
        """ä»HTMLæ–‡ä»¶åˆ—è¡¨ä¸­æ‰¾åˆ°ä¸»æ–‡ä»¶"""
        # ä¼˜å…ˆçº§ï¼šindex.html > game.html > main.html > å…¶ä»–
        priority_names = ['index.html', 'game.html', 'main.html', 'start.html']
        
        for priority_name in priority_names:
            for html_file in html_files:
                if os.path.basename(html_file).lower() == priority_name:
                    return html_file
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¼˜å…ˆæ–‡ä»¶ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
        return html_files[0] if html_files else None
    
    def find_iframe_games(self, game_url: str, game_title: str) -> Optional[Dict]:
        """æŸ¥æ‰¾å¯åµŒå…¥çš„iframeæ¸¸æˆ"""
        try:
            response = self.session.get(game_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾iframe
            iframes = soup.select('iframe[src]')
            for iframe in iframes:
                iframe_src = iframe.get('src')
                if iframe_src:
                    iframe_url = urljoin(game_url, iframe_src)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¯åµŒå…¥çš„åŸŸå
                    parsed = urlparse(iframe_url)
                    if any(domain in parsed.netloc for domain in EMBEDDABLE_DOMAINS):
                        logger.info(f"âœ… æ‰¾åˆ°å¯åµŒå…¥æ¸¸æˆ: {game_title}")
                        return {
                            'type': 'iframe',
                            'iframeUrl': iframe_url
                        }
            
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾iframeæ¸¸æˆå¤±è´¥: {e}")
            return None
    
    def crawl_premium_sites(self, max_games_per_site: int = 10) -> List[Dict]:
        """çˆ¬å–é«˜è´¨é‡æ¸¸æˆç½‘ç«™"""
        all_games = []
        
        for site in PREMIUM_GAME_SITES:
            try:
                logger.info(f"æ­£åœ¨çˆ¬å–: {site['name']}")
                response = self.session.get(site['search_url'], timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                game_elements = soup.select(site['game_selector'])[:max_games_per_site]
                
                for i, element in enumerate(game_elements):
                    try:
                        # æå–æ¸¸æˆä¿¡æ¯
                        title_elem = element.select_one(site['title_selector'])
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        if len(title) < 3:
                            continue
                        
                        # è·å–æ¸¸æˆé“¾æ¥
                        link_elem = element.select_one('a[href]')
                        if not link_elem:
                            continue
                        
                        game_url = urljoin(site['base_url'], link_elem['href'])
                        game_id = f"{site['name'].lower().replace(' ', '_')}_{i+1}"
                        
                        # ä¼˜å…ˆå°è¯•ä¸‹è½½åˆ°æœ¬åœ°
                        game_data = self.download_html5_game(game_url, title, game_id)
                        
                        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå°è¯•iframe
                        if not game_data:
                            game_data = self.find_iframe_games(game_url, title)
                        
                        if game_data:
                            game_info = {
                                'id': game_id,
                                'title': title,
                                'description': f"æ¥è‡ª{site['name']}çš„HTML5æ¸¸æˆ",
                                'category': 'ä¼‘é—²',
                                'categoryId': '1',
                                'thumbnail': '/games/thumbnails/default.jpg',
                                'path': f'/games/{game_id}',
                                'featured': False,
                                'addedAt': datetime.now().strftime('%Y-%m-%d'),
                                'tags': ['HTML5', 'åœ¨çº¿'],
                                'source': site['name'],
                                **game_data
                            }
                            all_games.append(game_info)
                            
                            if game_data['type'] == 'static':
                                self.downloaded_games.append(game_info)
                            else:
                                self.iframe_games.append(game_info)
                        
                        time.sleep(random.uniform(2, 4))  # é¿å…è¯·æ±‚è¿‡å¿«
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†æ¸¸æˆå¤±è´¥: {e}")
                        continue
                
                time.sleep(random.uniform(5, 8))  # ç½‘ç«™é—´å»¶è¿Ÿ
                
            except Exception as e:
                logger.error(f"çˆ¬å–ç½‘ç«™å¤±è´¥ {site['name']}: {e}")
                continue
        
        return all_games
    
    def save_games_data(self, games: List[Dict]):
        """ä¿å­˜æ¸¸æˆæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            json_file = os.path.join(PROJECT_ROOT, 'scripts', 'advanced_games.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(games, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ä¿å­˜ {len(games)} ä¸ªæ¸¸æˆåˆ° {json_file}")
            logger.info(f"ğŸ“ æœ¬åœ°æ¸¸æˆ: {len(self.downloaded_games)} ä¸ª")
            logger.info(f"ğŸŒ iframeæ¸¸æˆ: {len(self.iframe_games)} ä¸ª")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¸¸æˆæ•°æ®å¤±è´¥: {e}")
    
    def run(self, max_games: int = 30):
        """è¿è¡Œé«˜çº§çˆ¬è™«"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œé«˜çº§æ¸¸æˆçˆ¬è™«...")
        logger.info("ğŸ’¡ ä¼˜å…ˆä¸‹è½½HTML5æ¸¸æˆåˆ°æœ¬åœ°ï¼Œå…¶æ¬¡è€ƒè™‘iframeåµŒå…¥")
        
        # 1. çˆ¬å–é«˜è´¨é‡æ¸¸æˆç½‘ç«™
        games = self.crawl_premium_sites(max_games_per_site=max_games//len(PREMIUM_GAME_SITES))
        
        # 2. å¦‚æœé…ç½®äº†APIï¼Œè¿›è¡Œé¢å¤–æœç´¢
        if SERPAPI_KEY or (GOOGLE_API_KEY and GOOGLE_CX):
            logger.info("ğŸ” ä½¿ç”¨APIè¿›è¡Œé¢å¤–æœç´¢...")
            api_queries = [
                'HTML5 games download',
                'browser games source code',
                'free HTML5 games'
            ]
            
            for query in api_queries:
                try:
                    urls = self.search_with_api(query, 5)
                    # å¤„ç†APIæœç´¢ç»“æœ...
                    time.sleep(random.uniform(3, 5))
                except Exception as e:
                    logger.error(f"APIæœç´¢å¤±è´¥: {e}")
        
        # 3. ä¿å­˜ç»“æœ
        if games:
            self.save_games_data(games)
            logger.info(f"âœ… é«˜çº§çˆ¬è™«å®Œæˆï¼æ€»å…±å¤„ç† {len(games)} ä¸ªæ¸¸æˆ")
        else:
            logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¸¸æˆ")

def main():
    """ä¸»å‡½æ•°"""
    crawler = AdvancedGameCrawler()
    crawler.run(max_games=20)

if __name__ == '__main__':
    main() 