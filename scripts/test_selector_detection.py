#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€‰æ‹©å™¨æ£€æµ‹æµ‹è¯•å·¥å…·
ç”¨äºŽæµ‹è¯•æ–°æ¸¸æˆå¹³å°çš„è‡ªåŠ¨é€‰æ‹©å™¨æ£€æµ‹åŠŸèƒ½
"""

import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import logging

# å¯¼å…¥å…±äº«é…ç½®
from config import Config, get_proxy_settings

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_selector_detection(url: str, site_name: str = None):
    """æµ‹è¯•æŒ‡å®šç½‘ç«™çš„é€‰æ‹©å™¨æ£€æµ‹"""
    if not site_name:
        parsed = urlparse(url)
        site_name = parsed.netloc
    
    logger.info(f"ðŸ” æµ‹è¯•ç½‘ç«™: {site_name}")
    logger.info(f"ðŸŒ URL: {url}")
    
    try:
        # å¯¼å…¥GameManagerï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªçŽ¯ä¾èµ–ï¼‰
        from game_manager import GameManager, get_random_headers
        
        # åˆ›å»ºGameManagerå®žä¾‹
        manager = GameManager()
        
        # èŽ·å–é¡µé¢å†…å®¹
        headers = get_random_headers()
        proxies = get_proxy_settings()
        response = requests.get(url, headers=headers, proxies=proxies, timeout=Config.REQUEST_TIMEOUT)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        logger.info(f"âœ… é¡µé¢åŠ è½½æˆåŠŸï¼Œå¤§å°: {len(response.text)} å­—ç¬¦")
        
        # æ‰§è¡Œé€‰æ‹©å™¨æ£€æµ‹
        detected_selectors = manager._detect_game_selectors(soup, site_name)
        
        if detected_selectors:
            game_selector = detected_selectors.get('game_selector')
            title_selector = detected_selectors.get('title_selector')
            
            logger.info("ðŸŽ¯ æ£€æµ‹ç»“æžœ:")
            logger.info(f"  æ¸¸æˆå®¹å™¨é€‰æ‹©å™¨: {game_selector}")
            logger.info(f"  æ ‡é¢˜é€‰æ‹©å™¨: {title_selector}")
            
            # éªŒè¯æ£€æµ‹ç»“æžœ
            logger.info("ðŸ§ª éªŒè¯æ£€æµ‹ç»“æžœ...")
            game_elements = soup.select(game_selector)
            logger.info(f"  æ‰¾åˆ° {len(game_elements)} ä¸ªæ¸¸æˆå…ƒç´ ")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ¸¸æˆçš„æ ‡é¢˜
            for i, element in enumerate(game_elements[:5]):
                try:
                    title_elem = element.select_one(title_selector)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link_elem = element.select_one('a[href]') or (element if element.name == 'a' else None)
                        game_url = ""
                        if link_elem and link_elem.get('href'):
                            game_url = urljoin(url, link_elem['href'])
                        
                        logger.info(f"  æ¸¸æˆ {i+1}: {title}")
                        if game_url:
                            logger.info(f"    é“¾æŽ¥: {game_url}")
                except Exception as e:
                    logger.warning(f"  æ¸¸æˆ {i+1}: è§£æžå¤±è´¥ - {e}")
            
            # ç”Ÿæˆé…ç½®å»ºè®®
            config_suggestion = {
                'name': site_name,
                'base_url': f"{urlparse(url).scheme}://{urlparse(url).netloc}",
                'search_url': url,
                'game_selector': game_selector,
                'title_selector': title_selector,
                'priority': 5
            }
            
            logger.info("ðŸ“‹ å»ºè®®çš„é…ç½®:")
            print(json.dumps(config_suggestion, indent=2, ensure_ascii=False))
            
        else:
            logger.error("âŒ æœªèƒ½æ£€æµ‹åˆ°åˆé€‚çš„é€‰æ‹©å™¨")
            logger.info("ðŸ” æ‰‹åŠ¨åˆ†æžé¡µé¢ç»“æž„...")
            
            # æä¾›ä¸€äº›æ‰‹åŠ¨åˆ†æžçš„ä¿¡æ¯
            analyze_page_structure(soup, url)
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def analyze_page_structure(soup: BeautifulSoup, url: str):
    """æ‰‹åŠ¨åˆ†æžé¡µé¢ç»“æž„ï¼Œæä¾›è°ƒè¯•ä¿¡æ¯"""
    logger.info("ðŸ“Š é¡µé¢ç»“æž„åˆ†æž:")
    
    # åˆ†æžå¸¸è§çš„å®¹å™¨ç±»å
    common_classes = {}
    for element in soup.find_all(class_=True):
        for class_name in element.get('class', []):
            if any(keyword in class_name.lower() for keyword in ['game', 'item', 'card', 'entry', 'product']):
                common_classes[class_name] = common_classes.get(class_name, 0) + 1
    
    if common_classes:
        logger.info("  å¸¸è§çš„æ¸¸æˆç›¸å…³class:")
        for class_name, count in sorted(common_classes.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"    .{class_name}: {count} ä¸ªå…ƒç´ ")
    
    # åˆ†æžé“¾æŽ¥æ¨¡å¼
    links = soup.find_all('a', href=True)
    game_links = []
    for link in links[:20]:  # åªåˆ†æžå‰20ä¸ªé“¾æŽ¥
        href = link.get('href', '')
        text = link.get_text(strip=True)
        if text and len(text) > 2 and len(text) < 100:
            full_url = urljoin(url, href)
            game_links.append((text, full_url))
    
    if game_links:
        logger.info("  å¯èƒ½çš„æ¸¸æˆé“¾æŽ¥:")
        for text, link_url in game_links[:5]:
            logger.info(f"    {text}: {link_url}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ¸¸æˆç½‘ç«™çš„é€‰æ‹©å™¨è‡ªåŠ¨æ£€æµ‹')
    parser.add_argument('url', help='è¦æµ‹è¯•çš„ç½‘ç«™URL')
    parser.add_argument('--name', help='ç½‘ç«™åç§°ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    test_selector_detection(args.url, args.name)

if __name__ == '__main__':
    main() 