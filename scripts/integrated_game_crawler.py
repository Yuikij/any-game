#!/usr/bin/env python3
"""
é›†æˆæ¸¸æˆçˆ¬è™« - å®Œæ•´ç‰ˆ
æ•´åˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ï¼šå¤šå¹³å°çˆ¬å–ã€APIæœç´¢ã€ä»£ç†æ”¯æŒã€ç¼©ç•¥å›¾ç”Ÿæˆç­‰

ä½¿ç”¨æ–¹æ³•:
    python integrated_game_crawler.py [é€‰é¡¹]

é€‰é¡¹:
    --mode MODE           çˆ¬å–æ¨¡å¼ (quick/full/api) [é»˜è®¤: full]
    --target TARGET       ç›®æ ‡æ¸¸æˆæ•°é‡ [é»˜è®¤: 50]
    --use-proxy          å¯ç”¨ä»£ç†
    --generate-thumbnails ç”Ÿæˆç¼©ç•¥å›¾
    --api-search         å¯ç”¨APIæœç´¢
    --platforms PLATFORMS æŒ‡å®šå¹³å°ï¼Œç”¨é€—å·åˆ†éš” [é»˜è®¤: all]
    --delay DELAY        è¯·æ±‚å»¶è¿Ÿ(ç§’) [é»˜è®¤: 0.5-1.0]
    --workers WORKERS    å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° [é»˜è®¤: 3]
    --help               æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
    # å¿«é€Ÿæ¨¡å¼ï¼Œçˆ¬å–30ä¸ªæ¸¸æˆ
    python integrated_game_crawler.py --mode quick --target 30
    
    # å®Œæ•´æ¨¡å¼ï¼Œä½¿ç”¨ä»£ç†å’ŒAPIæœç´¢
    python integrated_game_crawler.py --mode full --use-proxy --api-search
    
    # ä»…ä»ç‰¹å®šå¹³å°çˆ¬å–
    python integrated_game_crawler.py --platforms itch.io,newgrounds
"""

import sys
import os
import json
import time
import random
import asyncio
import aiohttp
import logging
import argparse
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Set, Any, Optional, Union
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass, asdict
from datetime import datetime

import requests
from bs4 import BeautifulSoup
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('game_crawler.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class GameInfo:
    """æ¸¸æˆä¿¡æ¯æ•°æ®ç±»"""
    title: str
    url: str
    iframe_url: str
    description: str
    thumbnail: str
    category: str
    tags: List[str]
    platform: str
    score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

class ProxyManager:
    """ä»£ç†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.proxies = []
        self.current_proxy_index = 0
        self.load_proxies()
    
    def load_proxies(self):
        """åŠ è½½ä»£ç†åˆ—è¡¨"""
        proxy_file = "config/proxies.txt"
        if os.path.exists(proxy_file):
            with open(proxy_file, 'r') as f:
                self.proxies = [line.strip() for line in f if line.strip()]
            logger.info(f"åŠ è½½äº† {len(self.proxies)} ä¸ªä»£ç†")
    
    def get_proxy(self) -> Optional[Dict[str, str]]:
        """è·å–ä»£ç†"""
        if not self.proxies:
            return None
        
        proxy = self.proxies[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxies)
        
        return {
            'http': proxy,
            'https': proxy
        }

class APISearcher:
    """APIæœç´¢å™¨"""
    
    def __init__(self, use_proxy: bool = False):
        self.proxy_manager = ProxyManager() if use_proxy else None
        self.serp_api_key = self._load_api_key()
    
    def _load_api_key(self) -> Optional[str]:
        """åŠ è½½APIå¯†é’¥"""
        api_file = "config/api_keys.json"
        if os.path.exists(api_file):
            with open(api_file, 'r') as f:
                keys = json.load(f)
                return keys.get('serp_api_key')
        return None
    
    def search_games_via_api(self, query: str, num_results: int = 20) -> List[GameInfo]:
        """é€šè¿‡APIæœç´¢æ¸¸æˆ"""
        if not self.serp_api_key:
            logger.warning("SerpAPIå¯†é’¥æœªé…ç½®")
            return []
        
        try:
            params = {
                'engine': 'google',
                'q': f'{query} HTML5 games site:itch.io OR site:newgrounds.com',
                'api_key': self.serp_api_key,
                'num': num_results
            }
            
            proxies = self.proxy_manager.get_proxy() if self.proxy_manager else None
            response = requests.get('https://serpapi.com/search', params=params, proxies=proxies)
            
            if response.status_code == 200:
                data = response.json()
                games = []
                
                for result in data.get('organic_results', [])[:num_results]:
                    game = GameInfo(
                        title=result.get('title', ''),
                        url=result.get('link', ''),
                        iframe_url=result.get('link', ''),
                        description=result.get('snippet', ''),
                        thumbnail='/games/thumbnails/default.jpg',
                        category='ä¼‘é—²',
                        tags=['APIæœç´¢', 'HTML5', 'SerpAPI'],
                        platform='APIæœç´¢',
                        score=7.0
                    )
                    games.append(game)
                
                logger.info(f"APIæœç´¢æ‰¾åˆ° {len(games)} ä¸ªæ¸¸æˆ")
                return games
                
        except Exception as e:
            logger.error(f"APIæœç´¢å¤±è´¥: {e}")
        
        return []

class ThumbnailGenerator:
    """ç¼©ç•¥å›¾ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.colors = [
            [(66, 165, 245), (33, 150, 243)],   # è“è‰²
            [(102, 187, 106), (76, 175, 80)],  # ç»¿è‰²
            [(255, 167, 38), (255, 152, 0)],   # æ©™è‰²
            [(171, 71, 188), (156, 39, 176)],  # ç´«è‰²
            [(239, 83, 80), (244, 67, 54)],    # çº¢è‰²
            [(38, 198, 218), (0, 188, 212)],   # é’è‰²
            [(255, 238, 88), (255, 235, 59)],  # é»„è‰²
            [(158, 158, 158), (117, 117, 117)] # ç°è‰²
        ]
    
    def generate_thumbnail(self, title: str, index: int, output_path: str) -> bool:
        """ç”Ÿæˆå•ä¸ªç¼©ç•¥å›¾"""
        if not PIL_AVAILABLE:
            logger.warning("PILä¸å¯ç”¨ï¼Œè·³è¿‡ç¼©ç•¥å›¾ç”Ÿæˆ")
            return False
        
        try:
            # åˆ›å»ºå›¾åƒ
            width, height = 300, 200
            img = Image.new('RGB', (width, height), 'white')
            draw = ImageDraw.Draw(img)
            
            # é€‰æ‹©é¢œè‰²
            color_pair = self.colors[index % len(self.colors)]
            
            # ç»˜åˆ¶æ¸å˜èƒŒæ™¯
            for y in range(height):
                ratio = y / height
                r = int(color_pair[0][0] * (1 - ratio) + color_pair[1][0] * ratio)
                g = int(color_pair[0][1] * (1 - ratio) + color_pair[1][1] * ratio)
                b = int(color_pair[0][2] * (1 - ratio) + color_pair[1][2] * ratio)
                draw.line([(0, y), (width, y)], fill=(r, g, b))
            
            # æ·»åŠ å‡ ä½•å›¾æ¡ˆ
            pattern_type = index % 4
            if pattern_type == 0:  # åœ†å½¢
                draw.ellipse([50, 50, 150, 150], outline='white', width=3)
            elif pattern_type == 1:  # çŸ©å½¢
                draw.rectangle([50, 50, 150, 150], outline='white', width=3)
            elif pattern_type == 2:  # ä¸‰è§’å½¢
                draw.polygon([(100, 50), (50, 150), (150, 150)], outline='white', width=3)
            else:  # è±å½¢
                draw.polygon([(100, 50), (150, 100), (100, 150), (50, 100)], outline='white', width=3)
            
            # æ·»åŠ æ ‡é¢˜æ–‡å­—
            try:
                font = ImageFont.truetype("arial.ttf", 16)
            except:
                font = ImageFont.load_default()
            
            # å¤„ç†æ ‡é¢˜é•¿åº¦
            if len(title) > 20:
                title = title[:17] + "..."
            
            # è®¡ç®—æ–‡å­—ä½ç½®
            bbox = draw.textbbox((0, 0), title, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            text_x = (width - text_width) // 2
            text_y = height - text_height - 20
            
            # ç»˜åˆ¶æ–‡å­—é˜´å½±
            draw.text((text_x + 1, text_y + 1), title, font=font, fill='black')
            # ç»˜åˆ¶æ–‡å­—
            draw.text((text_x, text_y), title, font=font, fill='white')
            
            # ä¿å­˜å›¾åƒ
            img.save(output_path, 'JPEG', quality=85, optimize=True)
            return True
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥ {output_path}: {e}")
            return False
    
    def generate_batch_thumbnails(self, count: int) -> int:
        """æ‰¹é‡ç”Ÿæˆç¼©ç•¥å›¾"""
        if not PIL_AVAILABLE:
            logger.warning("PILä¸å¯ç”¨ï¼Œè·³è¿‡ç¼©ç•¥å›¾ç”Ÿæˆ")
            return 0
        
        thumbnail_dir = "public/games/thumbnails"
        os.makedirs(thumbnail_dir, exist_ok=True)
        
        generated = 0
        for i in range(1, count + 1):
            filename = f"auto_game_{i:03d}.jpg"
            output_path = os.path.join(thumbnail_dir, filename)
            
            if not os.path.exists(output_path):
                title = f"Game {i}"
                if self.generate_thumbnail(title, i, output_path):
                    generated += 1
                    if generated % 10 == 0:
                        logger.info(f"å·²ç”Ÿæˆ {generated} ä¸ªç¼©ç•¥å›¾...")
        
        logger.info(f"æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {generated} ä¸ªç¼©ç•¥å›¾")
        return generated

class PlatformCrawler:
    """å¹³å°çˆ¬è™«åŸºç±»"""
    
    def __init__(self, use_proxy: bool = False, delay_range: tuple = (0.5, 1.0)):
        self.session = requests.Session()
        self.proxy_manager = ProxyManager() if use_proxy else None
        self.delay_range = delay_range
        self.setup_headers()
    
    def setup_headers(self):
        """è®¾ç½®è¯·æ±‚å¤´"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_page(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            proxies = self.proxy_manager.get_proxy() if self.proxy_manager else None
            response = self.session.get(url, proxies=proxies, timeout=10)
            
            if response.status_code == 200:
                return BeautifulSoup(response.content, 'html.parser')
            else:
                logger.warning(f"HTTP {response.status_code}: {url}")
                return None
                
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)

class ItchIoCrawler(PlatformCrawler):
    """itch.ioçˆ¬è™«"""
    
    PLATFORM_NAME = "itch.io"
    SEARCH_URLS = [
        "https://itch.io/games/html5",
        "https://itch.io/games/html5/newest",
        "https://itch.io/games/html5/featured",
        "https://itch.io/games/html5/free"
    ]
    
    def crawl_games(self, max_games: int = 30) -> List[GameInfo]:
        """çˆ¬å–æ¸¸æˆ"""
        all_games = []
        
        for url in self.SEARCH_URLS:
            if len(all_games) >= max_games:
                break
                
            logger.info(f"ğŸ® çˆ¬å– {url}")
            soup = self.get_page(url)
            
            if soup:
                games = self.extract_games_from_page(soup, url)
                all_games.extend(games)
                logger.info(f"âœ… ä» {url} æ‰¾åˆ° {len(games)} ä¸ªæ¸¸æˆ")
            
            self.random_delay()
        
        return all_games[:max_games]
    
    def extract_games_from_page(self, soup: BeautifulSoup, page_url: str) -> List[GameInfo]:
        """ä»é¡µé¢æå–æ¸¸æˆä¿¡æ¯"""
        games = []
        
        # å¤šç§é€‰æ‹©å™¨å°è¯•
        selectors = [
            '.game_cell',
            '.game_thumb',
            '.game_link',
            'a[href*="/games/"]'
        ]
        
        for selector in selectors:
            game_elements = soup.select(selector)
            if game_elements:
                break
        
        for element in game_elements:
            try:
                game = self.extract_game_info(element, page_url)
                if game and self.validate_game(game):
                    games.append(game)
            except Exception as e:
                logger.debug(f"æå–æ¸¸æˆä¿¡æ¯å¤±è´¥: {e}")
                continue
        
        return games
    
    def extract_game_info(self, element, page_url: str) -> Optional[GameInfo]:
        """æå–å•ä¸ªæ¸¸æˆä¿¡æ¯"""
        try:
            # è·å–æ¸¸æˆé“¾æ¥
            link_elem = element.find('a') if element.name != 'a' else element
            if not link_elem:
                return None
            
            game_url = urljoin(page_url, link_elem.get('href', ''))
            if not game_url or '/games/' not in game_url:
                return None
            
            # è·å–æ ‡é¢˜
            title_elem = (element.select_one('.title') or 
                         element.select_one('.game_title') or
                         element.select_one('img') or
                         link_elem)
            
            title = ""
            if title_elem:
                if title_elem.name == 'img':
                    title = title_elem.get('alt', title_elem.get('title', ''))
                else:
                    title = title_elem.get_text(strip=True)
            
            if not title:
                return None
            
            # æ¨æ–­iframe URL
            iframe_url = self.infer_iframe_url(game_url)
            
            # è·å–æè¿°
            desc_elem = element.select_one('.game_text, .desc, .description')
            description = desc_elem.get_text(strip=True) if desc_elem else f"æ¥è‡ª{self.PLATFORM_NAME}çš„HTML5æ¸¸æˆ"
            
            return GameInfo(
                title=title,
                url=game_url,
                iframe_url=iframe_url,
                description=description,
                thumbnail='/games/thumbnails/default.jpg',
                category='ä¼‘é—²',
                tags=['HTML5', 'åœ¨çº¿', self.PLATFORM_NAME],
                platform=self.PLATFORM_NAME,
                score=self.calculate_game_score(title, description)
            )
            
        except Exception as e:
            logger.debug(f"æå–æ¸¸æˆä¿¡æ¯å¼‚å¸¸: {e}")
            return None
    
    def infer_iframe_url(self, game_url: str) -> str:
        """æ¨æ–­iframe URL"""
        try:
            # å°è¯•è®¿é—®æ¸¸æˆé¡µé¢è·å–çœŸå®iframe
            soup = self.get_page(game_url)
            if soup:
                iframe = soup.select_one('iframe[src*="html-classic.itch.zone"]')
                if iframe:
                    return iframe.get('src')
            
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨æ¨¡å¼æ¨æ–­
            if 'itch.io' in game_url:
                game_id = game_url.split('/')[-1]
                return f"https://html-classic.itch.zone/html/{game_id}/index.html"
            
        except:
            pass
        
        return game_url
    
    def validate_game(self, game: GameInfo) -> bool:
        """éªŒè¯æ¸¸æˆä¿¡æ¯"""
        return (game.title and 
                len(game.title) >= 2 and 
                game.url and 
                game.iframe_url)
    
    def calculate_game_score(self, title: str, description: str) -> float:
        """è®¡ç®—æ¸¸æˆè¯„åˆ†"""
        score = 5.0
        
        # æ ‡é¢˜è´¨é‡
        if len(title) > 5:
            score += 1.0
        if len(title) < 50:
            score += 0.5
        
        # æè¿°è´¨é‡
        if len(description) > 20:
            score += 1.0
        
        # å…³é”®è¯å¥–åŠ±
        quality_keywords = ['game', 'play', 'adventure', 'puzzle', 'action']
        if any(keyword in title.lower() or keyword in description.lower() for keyword in quality_keywords):
            score += 0.5
        
        return min(score, 10.0)

class NewgroundsCrawler(PlatformCrawler):
    """Newgroundsçˆ¬è™«"""
    
    PLATFORM_NAME = "Newgrounds"
    SEARCH_URLS = [
        "https://www.newgrounds.com/games/browse",
        "https://www.newgrounds.com/games/browse/newest",
        "https://www.newgrounds.com/games/browse/featured"
    ]
    
    def crawl_games(self, max_games: int = 20) -> List[GameInfo]:
        """çˆ¬å–æ¸¸æˆ"""
        all_games = []
        
        for url in self.SEARCH_URLS:
            if len(all_games) >= max_games:
                break
                
            logger.info(f"ğŸ® çˆ¬å– {url}")
            soup = self.get_page(url)
            
            if soup:
                games = self.extract_games_from_page(soup, url)
                all_games.extend(games)
                logger.info(f"âœ… ä» {url} æ‰¾åˆ° {len(games)} ä¸ªæ¸¸æˆ")
            
            self.random_delay()
        
        return all_games[:max_games]
    
    def extract_games_from_page(self, soup: BeautifulSoup, page_url: str) -> List[GameInfo]:
        """ä»é¡µé¢æå–æ¸¸æˆä¿¡æ¯"""
        games = []
        
        # Newgroundsç‰¹å®šé€‰æ‹©å™¨
        selectors = [
            '.item-portalitem',
            '.portal-item',
            'a[href*="/portal/view/"]'
        ]
        
        for selector in selectors:
            game_elements = soup.select(selector)
            if game_elements:
                break
        
        for element in game_elements:
            try:
                game = self.extract_game_info(element, page_url)
                if game and self.validate_game(game):
                    games.append(game)
            except Exception as e:
                logger.debug(f"æå–æ¸¸æˆä¿¡æ¯å¤±è´¥: {e}")
                continue
        
        return games
    
    def extract_game_info(self, element, page_url: str) -> Optional[GameInfo]:
        """æå–å•ä¸ªæ¸¸æˆä¿¡æ¯"""
        try:
            # è·å–æ¸¸æˆé“¾æ¥
            link_elem = element.find('a') if element.name != 'a' else element
            if not link_elem:
                return None
            
            game_url = urljoin(page_url, link_elem.get('href', ''))
            if not game_url or '/portal/view/' not in game_url:
                return None
            
            # è·å–æ ‡é¢˜
            title_elem = (element.select_one('.item-details h4') or 
                         element.select_one('.title') or
                         element.select_one('img'))
            
            title = ""
            if title_elem:
                if title_elem.name == 'img':
                    title = title_elem.get('alt', title_elem.get('title', ''))
                else:
                    title = title_elem.get_text(strip=True)
            
            if not title:
                return None
            
            # è·å–æè¿°
            desc_elem = element.select_one('.item-details p, .description')
            description = desc_elem.get_text(strip=True) if desc_elem else f"æ¥è‡ª{self.PLATFORM_NAME}çš„æ¸¸æˆ"
            
            return GameInfo(
                title=title,
                url=game_url,
                iframe_url=game_url,  # Newgroundsé€šå¸¸ç›´æ¥åµŒå…¥
                description=description,
                thumbnail='/games/thumbnails/default.jpg',
                category='ä¼‘é—²',
                tags=['Flash', 'åœ¨çº¿', self.PLATFORM_NAME],
                platform=self.PLATFORM_NAME,
                score=6.0
            )
            
        except Exception as e:
            logger.debug(f"æå–æ¸¸æˆä¿¡æ¯å¼‚å¸¸: {e}")
            return None
    
    def validate_game(self, game: GameInfo) -> bool:
        """éªŒè¯æ¸¸æˆä¿¡æ¯"""
        return (game.title and 
                len(game.title) >= 2 and 
                game.url)

class IntegratedGameCrawler:
    """é›†æˆæ¸¸æˆçˆ¬è™«"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.use_proxy = config.get('use_proxy', False)
        self.generate_thumbnails = config.get('generate_thumbnails', True)
        self.api_search = config.get('api_search', False)
        self.delay_range = config.get('delay_range', (0.5, 1.0))
        self.workers = config.get('workers', 3)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.thumbnail_generator = ThumbnailGenerator()
        self.api_searcher = APISearcher(self.use_proxy) if self.api_search else None
        
        # åˆå§‹åŒ–å¹³å°çˆ¬è™«
        self.platform_crawlers = {
            'itch.io': ItchIoCrawler(self.use_proxy, self.delay_range),
            'newgrounds': NewgroundsCrawler(self.use_proxy, self.delay_range)
        }
        
        # åŠ è½½ç°æœ‰æ¸¸æˆ
        self.existing_games = self.load_existing_games()
    
    def load_existing_games(self) -> Set[str]:
        """åŠ è½½ç°æœ‰æ¸¸æˆæ ‡é¢˜"""
        existing = set()
        games_file = "src/data/games.ts"
        
        if os.path.exists(games_file):
            with open(games_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–ç°æœ‰æ¸¸æˆæ ‡é¢˜
                import re
                titles = re.findall(r"title: '([^']+)'", content)
                existing.update(title.lower().strip() for title in titles)
        
        logger.info(f"åŠ è½½äº† {len(existing)} ä¸ªç°æœ‰æ¸¸æˆæ ‡é¢˜")
        return existing
    
    def is_duplicate(self, game: GameInfo) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        title_lower = game.title.lower().strip()
        return title_lower in self.existing_games
    
    def crawl_platform(self, platform_name: str, max_games: int) -> List[GameInfo]:
        """çˆ¬å–å•ä¸ªå¹³å°"""
        if platform_name not in self.platform_crawlers:
            logger.warning(f"ä¸æ”¯æŒçš„å¹³å°: {platform_name}")
            return []
        
        logger.info(f"ğŸ® å¼€å§‹çˆ¬å–å¹³å°: {platform_name}")
        crawler = self.platform_crawlers[platform_name]
        games = crawler.crawl_games(max_games)
        
        # å»é‡
        unique_games = []
        for game in games:
            if not self.is_duplicate(game):
                unique_games.append(game)
                self.existing_games.add(game.title.lower().strip())
        
        logger.info(f"âœ… {platform_name} å®Œæˆï¼Œæ‰¾åˆ° {len(unique_games)} ä¸ªæ¸¸æˆ")
        return unique_games
    
    def crawl_all_platforms(self, target_games: int) -> List[GameInfo]:
        """çˆ¬å–æ‰€æœ‰å¹³å°"""
        all_games = []
        platforms = self.config.get('platforms', ['itch.io', 'newgrounds'])
        
        if isinstance(platforms, str):
            platforms = [p.strip() for p in platforms.split(',')]
        
        # è®¡ç®—æ¯ä¸ªå¹³å°çš„ç›®æ ‡æ•°é‡
        games_per_platform = max(1, target_games // len(platforms))
        
        # å¹¶å‘çˆ¬å–
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            future_to_platform = {
                executor.submit(self.crawl_platform, platform, games_per_platform): platform
                for platform in platforms if platform in self.platform_crawlers
            }
            
            for future in as_completed(future_to_platform):
                platform = future_to_platform[future]
                try:
                    games = future.result()
                    all_games.extend(games)
                    
                    if len(all_games) >= target_games:
                        # å–æ¶ˆå…¶ä»–ä»»åŠ¡
                        for f in future_to_platform:
                            if not f.done():
                                f.cancel()
                        break
                        
                except Exception as e:
                    logger.error(f"çˆ¬å–å¹³å° {platform} å¤±è´¥: {e}")
        
        return all_games[:target_games]
    
    def search_via_api(self, queries: List[str], target_games: int) -> List[GameInfo]:
        """é€šè¿‡APIæœç´¢æ¸¸æˆ"""
        if not self.api_searcher:
            return []
        
        all_games = []
        games_per_query = max(1, target_games // len(queries))
        
        for query in queries:
            games = self.api_searcher.search_games_via_api(query, games_per_query)
            
            # å»é‡
            for game in games:
                if not self.is_duplicate(game):
                    all_games.append(game)
                    self.existing_games.add(game.title.lower().strip())
            
            if len(all_games) >= target_games:
                break
        
        return all_games[:target_games]
    
    def generate_game_thumbnails(self, games: List[GameInfo]) -> None:
        """ç”Ÿæˆæ¸¸æˆç¼©ç•¥å›¾"""
        if not self.generate_thumbnails or not PIL_AVAILABLE:
            return
        
        logger.info("ğŸ¨ å¼€å§‹ç”Ÿæˆç¼©ç•¥å›¾...")
        
        # ç”Ÿæˆè¶³å¤Ÿçš„ç¼©ç•¥å›¾
        thumbnail_count = max(50, len(games) + 10)
        generated = self.thumbnail_generator.generate_batch_thumbnails(thumbnail_count)
        
        # ä¸ºæ¸¸æˆåˆ†é…ç¼©ç•¥å›¾
        self.assign_thumbnails_to_games(games)
        
        logger.info(f"âœ… ç¼©ç•¥å›¾ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {generated} ä¸ª")
    
    def assign_thumbnails_to_games(self, games: List[GameInfo]) -> None:
        """ä¸ºæ¸¸æˆåˆ†é…ç¼©ç•¥å›¾"""
        thumbnail_dir = "public/games/thumbnails"
        if not os.path.exists(thumbnail_dir):
            return
        
        # è·å–å¯ç”¨çš„è‡ªåŠ¨ç”Ÿæˆç¼©ç•¥å›¾
        auto_thumbnails = [
            f"/games/thumbnails/{f}" 
            for f in os.listdir(thumbnail_dir) 
            if f.startswith("auto_game_") and f.endswith(".jpg")
        ]
        auto_thumbnails.sort()
        
        # ä¸ºé»˜è®¤ç¼©ç•¥å›¾çš„æ¸¸æˆåˆ†é…æ–°ç¼©ç•¥å›¾
        thumbnail_index = 0
        for game in games:
            if game.thumbnail == '/games/thumbnails/default.jpg' and thumbnail_index < len(auto_thumbnails):
                game.thumbnail = auto_thumbnails[thumbnail_index]
                thumbnail_index += 1
    
    def save_games(self, games: List[GameInfo]) -> bool:
        """ä¿å­˜æ¸¸æˆæ•°æ®"""
        try:
            games_file = "src/data/games.ts"
            
            # å¤‡ä»½åŸæ–‡ä»¶
            if os.path.exists(games_file):
                backup_file = f"{games_file}.backup.{int(time.time())}"
                os.rename(games_file, backup_file)
                logger.info(f"ğŸ“ å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
            
            # è¯»å–ç°æœ‰æ•°æ®
            existing_games_data = self.parse_existing_games()
            
            # åˆå¹¶æ–°æ¸¸æˆ
            all_games_data = existing_games_data + [self.convert_to_game_dict(game, len(existing_games_data) + i) for i, game in enumerate(games)]
            
            # ç”Ÿæˆæ–°çš„games.tsæ–‡ä»¶
            self.write_games_file(all_games_data)
            
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(all_games_data)} ä¸ªæ¸¸æˆåˆ° {games_file}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¸¸æˆæ•°æ®å¤±è´¥: {e}")
            return False
    
    def parse_existing_games(self) -> List[Dict[str, Any]]:
        """è§£æç°æœ‰æ¸¸æˆæ•°æ®"""
        games_file = "src/data/games.ts"
        if not os.path.exists(games_file):
            return []
        
        try:
            with open(games_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„æ­£åˆ™æå–ï¼ˆå¯ä»¥æ”¹è¿›ä¸ºæ›´å¥å£®çš„è§£æï¼‰
            import re
            games_match = re.search(r'export const games: Game\[\] = \[(.*?)\];', content, re.DOTALL)
            if not games_match:
                return []
            
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è§£æé€»è¾‘
            # ä¸ºç®€åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼Œè®©æ–°æ•°æ®è¿½åŠ 
            return []
            
        except Exception as e:
            logger.error(f"è§£æç°æœ‰æ¸¸æˆæ•°æ®å¤±è´¥: {e}")
            return []
    
    def convert_to_game_dict(self, game: GameInfo, index: int) -> Dict[str, Any]:
        """è½¬æ¢æ¸¸æˆä¿¡æ¯ä¸ºå­—å…¸æ ¼å¼"""
        timestamp = int(time.time())
        game_id = f"crawled_{timestamp}_{index}"
        
        return {
            'id': game_id,
            'title': game.title,
            'description': game.description,
            'category': game.category,
            'categoryId': '1',  # é»˜è®¤ä¸ºä¼‘é—²
            'thumbnail': game.thumbnail,
            'path': f'/games/{game_id}',
            'featured': False,
            'type': 'iframe',
            'iframeUrl': game.iframe_url,
            'addedAt': datetime.now().strftime('%Y-%m-%d'),
            'tags': game.tags
        }
    
    def escape_string_for_js(self, text: str) -> str:
        """è½¬ä¹‰å­—ç¬¦ä¸²ç”¨äºJavaScript/TypeScriptè¾“å‡º"""
        if not text:
            return text
        
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        text = text.replace('\\', '\\\\')  # åæ–œæ å¿…é¡»é¦–å…ˆè½¬ä¹‰
        text = text.replace("'", "\\'")    # å•å¼•å·
        text = text.replace('"', '\\"')    # åŒå¼•å·  
        text = text.replace('\n', '\\n')   # æ¢è¡Œç¬¦
        text = text.replace('\r', '\\r')   # å›è½¦ç¬¦
        text = text.replace('\t', '\\t')   # åˆ¶è¡¨ç¬¦
        
        return text
    
    def write_games_file(self, games_data: List[Dict[str, Any]]) -> None:
        """å†™å…¥æ¸¸æˆæ–‡ä»¶"""
        # ç»Ÿè®¡åˆ†ç±»
        category_counts = {}
        for game in games_data:
            category = game['category']
            category_counts[category] = category_counts.get(category, 0) + 1
        
        # ç”Ÿæˆæ–‡ä»¶å†…å®¹
        content = """import { Game, Category } from '../types';

export const categories: Category[] = [
  { id: '1', name: 'ä¼‘é—²', description: 'ç®€å•æœ‰è¶£çš„ä¼‘é—²æ¸¸æˆï¼Œé€‚åˆæ‰€æœ‰å¹´é¾„æ®µç©å®¶', count: """ + str(category_counts.get('ä¼‘é—²', 0)) + """, slug: 'casual' },
  { id: '2', name: 'ç›Šæ™º', description: 'é”»ç‚¼å¤§è„‘çš„ç›Šæ™ºæ¸¸æˆï¼Œæå‡æ€ç»´èƒ½åŠ›', count: """ + str(category_counts.get('ç›Šæ™º', 0)) + """, slug: 'puzzle' },
  { id: '3', name: 'åŠ¨ä½œ', description: 'åˆºæ¿€çš„åŠ¨ä½œæ¸¸æˆï¼Œè€ƒéªŒä½ çš„ååº”é€Ÿåº¦', count: """ + str(category_counts.get('åŠ¨ä½œ', 0)) + """, slug: 'action' },
  { id: '4', name: 'å¡ç‰Œ', description: 'å¡ç‰Œå’Œæ£‹ç‰Œç±»æ¸¸æˆï¼Œç­–ç•¥ä¸è¿æ°”çš„ç»“åˆ', count: """ + str(category_counts.get('å¡ç‰Œ', 0)) + """, slug: 'card' },
  { id: '5', name: 'ä½“è‚²', description: 'å„ç±»ä½“è‚²æ¨¡æ‹Ÿæ¸¸æˆï¼Œæ„Ÿå—ä½“è‚²ç«æŠ€çš„ä¹è¶£', count: """ + str(category_counts.get('ä½“è‚²', 0)) + """, slug: 'sports' },
  { id: '6', name: 'æ£‹ç›˜', description: 'ç»å…¸çš„æ£‹ç›˜æ¸¸æˆï¼Œè€ƒéªŒæˆ˜ç•¥æ€ç»´', count: """ + str(category_counts.get('æ£‹ç›˜', 0)) + """, slug: 'board' },
];

export const games: Game[] = [
"""
        
                                  # æ·»åŠ æ¸¸æˆæ•°æ®
        for i, game in enumerate(games_data):
            # å®‰å…¨å¤„ç†å­—ç¬¦ä¸²ï¼Œè½¬ä¹‰å•å¼•å·å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
            title = self.escape_string_for_js(game['title'])
            description = self.escape_string_for_js(game['description'])
            category = self.escape_string_for_js(game['category'])
            thumbnail = self.escape_string_for_js(game['thumbnail'])
            path = self.escape_string_for_js(game['path'])
            iframe_url = self.escape_string_for_js(game['iframeUrl'])
            
            content += f"""  {{
    id: '{game['id']}',
    title: '{title}',
    description: '{description}',
    category: '{category}',
    categoryId: '{game['categoryId']}',
    thumbnail: '{thumbnail}',
    path: '{path}',
    featured: {str(game['featured']).lower()},
    type: '{game['type']}',
    iframeUrl: '{iframe_url}',
    addedAt: '{game['addedAt']}',
    tags: {json.dumps(game['tags'], ensure_ascii=False)}
  }}"""
            
            if i < len(games_data) - 1:
                content += ","
            content += "\n"
        
        content += """];

export const getFeaturedGames = (): Game[] => {
  return games.filter(game => game.featured);
};

export const getRecentGames = (limit: number = 8): Game[] => {
  return games
    .sort((a, b) => new Date(b.addedAt).getTime() - new Date(a.addedAt).getTime())
    .slice(0, limit);
};

export const getGamesByCategory = (categoryId: string): Game[] => {
  return games.filter(game => game.categoryId === categoryId);
};

export const getGameById = (id: string): Game | undefined => {
  return games.find(game => game.id === id);
};

export const getCategoryById = (id: string): Category | undefined => {
  return categories.find(category => category.id === id);
};
"""
        
        # å†™å…¥æ–‡ä»¶
        with open("src/data/games.ts", 'w', encoding='utf-8') as f:
            f.write(content)
    
    def run(self, mode: str = 'full', target_games: int = 50) -> bool:
        """è¿è¡Œçˆ¬è™«"""
        logger.info(f"ğŸš€ å¯åŠ¨é›†æˆæ¸¸æˆçˆ¬è™« - æ¨¡å¼: {mode}, ç›®æ ‡: {target_games} ä¸ªæ¸¸æˆ")
        
        all_games = []
        
        try:
            # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒç­–ç•¥
            if mode == 'quick':
                # å¿«é€Ÿæ¨¡å¼ï¼šä¸»è¦ä»itch.ioçˆ¬å–
                all_games = self.crawl_platform('itch.io', target_games)
                
            elif mode == 'api':
                # APIæ¨¡å¼ï¼šä¸»è¦é€šè¿‡APIæœç´¢
                queries = ['HTML5 games', 'browser games', 'online games', 'web games']
                all_games = self.search_via_api(queries, target_games)
                
            elif mode == 'full':
                # å®Œæ•´æ¨¡å¼ï¼šå¤šå¹³å° + APIæœç´¢
                platform_games = target_games * 2 // 3  # 2/3 æ¥è‡ªå¹³å°çˆ¬å–
                api_games = target_games - len(all_games)  # å‰©ä½™æ¥è‡ªAPI
                
                # å¹³å°çˆ¬å–
                all_games.extend(self.crawl_all_platforms(platform_games))
                
                # APIæœç´¢è¡¥å……
                if self.api_search and len(all_games) < target_games:
                    queries = ['HTML5 games', 'browser games']
                    api_results = self.search_via_api(queries, api_games)
                    all_games.extend(api_results)
            
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")
                return False
            
            # å»é‡
            unique_games = []
            seen_titles = set()
            for game in all_games:
                title_lower = game.title.lower().strip()
                if title_lower not in seen_titles:
                    unique_games.append(game)
                    seen_titles.add(title_lower)
            
            logger.info(f"ğŸ¯ å»é‡åæ‰¾åˆ° {len(unique_games)} ä¸ªå”¯ä¸€æ¸¸æˆ")
            
            # ç”Ÿæˆç¼©ç•¥å›¾
            if self.generate_thumbnails:
                self.generate_game_thumbnails(unique_games)
            
            # ä¿å­˜æ¸¸æˆ
            if unique_games:
                success = self.save_games(unique_games)
                if success:
                    self.print_summary(unique_games)
                    return True
            else:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–°æ¸¸æˆ")
            
        except Exception as e:
            logger.error(f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        
        return False
    
    def print_summary(self, games: List[GameInfo]) -> None:
        """æ‰“å°æ‘˜è¦"""
        logger.info("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        
        # æŒ‰å¹³å°ç»Ÿè®¡
        platform_stats = {}
        for game in games:
            platform = game.platform
            platform_stats[platform] = platform_stats.get(platform, 0) + 1
        
        for platform, count in platform_stats.items():
            logger.info(f"  ğŸ® {platform}: {count} ä¸ªæ¸¸æˆ")
        
        # ç¼©ç•¥å›¾ç»Ÿè®¡
        thumbnail_stats = {
            'default': 0,
            'auto': 0,
            'custom': 0
        }
        
        for game in games:
            if 'default.jpg' in game.thumbnail:
                thumbnail_stats['default'] += 1
            elif 'auto_game_' in game.thumbnail:
                thumbnail_stats['auto'] += 1
            else:
                thumbnail_stats['custom'] += 1
        
        logger.info(f"  ğŸ–¼ï¸ è‡ªå®šä¹‰ç¼©ç•¥å›¾: {thumbnail_stats['custom']}/{len(games)} ä¸ª")
        logger.info("ğŸ‰ é›†æˆçˆ¬è™«æ‰§è¡Œå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='é›†æˆæ¸¸æˆçˆ¬è™« - å®Œæ•´ç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --mode quick --target 30
  %(prog)s --mode full --use-proxy --api-search
  %(prog)s --platforms itch.io,newgrounds --generate-thumbnails
        """
    )
    
    parser.add_argument('--mode', choices=['quick', 'full', 'api'], default='full',
                        help='çˆ¬å–æ¨¡å¼ (é»˜è®¤: full)')
    parser.add_argument('--target', type=int, default=50,
                        help='ç›®æ ‡æ¸¸æˆæ•°é‡ (é»˜è®¤: 50)')
    parser.add_argument('--use-proxy', action='store_true',
                        help='å¯ç”¨ä»£ç†')
    parser.add_argument('--generate-thumbnails', action='store_true', default=True,
                        help='ç”Ÿæˆç¼©ç•¥å›¾ (é»˜è®¤å¯ç”¨)')
    parser.add_argument('--api-search', action='store_true',
                        help='å¯ç”¨APIæœç´¢')
    parser.add_argument('--platforms', default='itch.io,newgrounds',
                        help='æŒ‡å®šå¹³å°ï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: itch.io,newgrounds)')
    parser.add_argument('--delay', default='0.5-1.0',
                        help='è¯·æ±‚å»¶è¿ŸèŒƒå›´(ç§’) (é»˜è®¤: 0.5-1.0)')
    parser.add_argument('--workers', type=int, default=3,
                        help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 3)')
    
    args = parser.parse_args()
    
    # è§£æå»¶è¿Ÿå‚æ•°
    delay_parts = args.delay.split('-')
    if len(delay_parts) == 2:
        delay_range = (float(delay_parts[0]), float(delay_parts[1]))
    else:
        delay_range = (0.5, 1.0)
    
    # æ„å»ºé…ç½®
    config = {
        'use_proxy': args.use_proxy,
        'generate_thumbnails': args.generate_thumbnails,
        'api_search': args.api_search,
        'platforms': args.platforms,
        'delay_range': delay_range,
        'workers': args.workers
    }
    
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    crawler = IntegratedGameCrawler(config)
    success = crawler.run(args.mode, args.target)
    
    if success:
        print("âœ… çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼")
        sys.exit(0)
    else:
        print("âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main() 